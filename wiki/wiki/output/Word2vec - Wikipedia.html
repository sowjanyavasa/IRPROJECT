<!DOCTYPE html>
<html class="client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-sticky-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-0 vector-feature-client-preferences-disabled vector-feature-client-prefs-pinned-disabled vector-feature-night-mode-disabled skin-theme-clientpref-day vector-toc-available" lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<title>Word2vec - Wikipedia</title>
<script>(function(){var className="client-js vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-sticky-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-0 vector-feature-client-preferences-disabled vector-feature-client-prefs-pinned-disabled vector-feature-night-mode-disabled skin-theme-clientpref-day vector-toc-available";var cookie=document.cookie.match(/(?:^|; )enwikimwclientpreferences=([^;]+)/);if(cookie){cookie[1].split('%2C').forEach(function(pref){className=className.replace(new RegExp('(^| )'+pref.replace(/-clientpref-\w+$|[^\w-]+/g,'')+'-clientpref-\\w+( |$)'),'$1'+pref+'$2');});}document.documentElement.className=className;}());RLCONF={"wgBreakFrames":false,"wgSeparatorTransformTable":["",""],
"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgRequestId":"0bb6414d-320d-45b9-b2d5-890ae2e9cf60","wgCanonicalNamespace":"","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":0,"wgPageName":"Word2vec","wgTitle":"Word2vec","wgCurRevisionId":1215655449,"wgRevisionId":1215655449,"wgArticleId":47527969,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["CS1 errors: missing periodical","Articles with short description","Short description is different from Wikidata","Wikipedia articles needing clarification from February 2022","Use dmy dates from April 2017","Free science software","Natural language processing toolkits","Artificial neural networks","Semantic relations"],"wgPageViewLanguage":"en","wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgRelevantPageName":
"Word2vec","wgRelevantArticleId":47527969,"wgIsProbablyEditable":true,"wgRelevantPageIsProbablyEditable":true,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgNoticeProject":"wikipedia","wgFlaggedRevsParams":{"tags":{"status":{"levels":1}}},"wgMediaViewerOnClick":true,"wgMediaViewerEnabledByDefault":true,"wgPopupsFlags":6,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en"},"wgMFDisplayWikibaseDescriptions":{"search":true,"watchlist":true,"tagline":false,"nearby":true},"wgWMESchemaEditAttemptStepOversample":false,"wgWMEPageLength":30000,"wgULSCurrentAutonym":"English","wgCentralAuthMobileDomain":false,"wgEditSubmitButtonLabelPublish":true,"wgULSPosition":"interlanguage","wgULSisCompactLinksEnabled":false,"wgVector2022LanguageInHeader":true,"wgULSisLanguageSelectorEmpty":false,"wgWikibaseItemId":"Q22673982","wgCheckUserClientHintsHeadersJsApi":["architecture","bitness","brands","fullVersionList","mobile","model","platform","platformVersion"],
"GEHomepageSuggestedEditsEnableTopics":true,"wgGETopicsMatchModeEnabled":false,"wgGEStructuredTaskRejectionReasonTextInputEnabled":false,"wgGELevelingUpEnabledForUser":false};RLSTATE={"skins.vector.user.styles":"ready","ext.globalCssJs.user.styles":"ready","site.styles":"ready","user.styles":"ready","skins.vector.user":"ready","ext.globalCssJs.user":"ready","user":"ready","user.options":"loading","ext.cite.styles":"ready","ext.math.styles":"ready","skins.vector.search.codex.styles":"ready","skins.vector.styles":"ready","skins.vector.icons":"ready","jquery.makeCollapsible.styles":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","wikibase.client.init":"ready","ext.wikimediaBadges":"ready"};RLPAGEMODULES=["ext.cite.ux-enhancements","mediawiki.page.media","site","mediawiki.page.ready","jquery.makeCollapsible","mediawiki.toc","skins.vector.js","ext.centralNotice.geoIP","ext.centralNotice.startUp","ext.gadget.ReferenceTooltips",
"ext.gadget.switcher","ext.urlShortener.toolbar","ext.centralauth.centralautologin","mmv.head","mmv.bootstrap.autostart","ext.popups","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.echo.centralauth","ext.eventLogging","ext.wikimediaEvents","ext.navigationTiming","ext.uls.interface","ext.cx.eventlogging.campaigns","ext.cx.uls.quick.actions","wikibase.client.vector-2022","ext.checkUser.clientHints","ext.growthExperiments.SuggestedEditSession"];</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.impl(function(){return["user.options@12s5i",function($,jQuery,require,module){mw.user.tokens.set({"patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});
}];});});</script>
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=ext.cite.styles%7Cext.math.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cjquery.makeCollapsible.styles%7Cskins.vector.icons%2Cstyles%7Cskins.vector.search.codex.styles%7Cwikibase.client.init&amp;only=styles&amp;skin=vector-2022">
<script async="" src="/w/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;raw=1&amp;skin=vector-2022"></script>
<meta name="ResourceLoaderDynamicStyles" content="">
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector-2022">
<meta name="generator" content="MediaWiki 1.42.0-wmf.26">
<meta name="referrer" content="origin">
<meta name="referrer" content="origin-when-cross-origin">
<meta name="robots" content="max-image-preview:standard">
<meta name="format-detection" content="telephone=no">
<meta name="viewport" content="width=1000">
<meta property="og:title" content="Word2vec - Wikipedia">
<meta property="og:type" content="website">
<link rel="preconnect" href="//upload.wikimedia.org">
<link rel="alternate" media="only screen and (max-width: 720px)" href="//en.m.wikipedia.org/wiki/Word2vec">
<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=Word2vec&amp;action=edit">
<link rel="apple-touch-icon" href="/static/apple-touch/wikipedia.png">
<link rel="icon" href="/static/favicon/wikipedia.ico">
<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)">
<link rel="EditURI" type="application/rsd+xml" href="//en.wikipedia.org/w/api.php?action=rsd">
<link rel="canonical" href="https://en.wikipedia.org/wiki/Word2vec">
<link rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/deed.en">
<link rel="alternate" type="application/atom+xml" title="Wikipedia Atom feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=atom">
<link rel="dns-prefetch" href="//meta.wikimedia.org" />
<link rel="dns-prefetch" href="//login.wikimedia.org">
</head>
<body class="skin-vector skin-vector-search-vue mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-Word2vec rootpage-Word2vec skin-vector-2022 action-view"><a class="mw-jump-link" href="#bodyContent">Jump to content</a>
<div class="vector-header-container">
	<header class="vector-header mw-header">
		<div class="vector-header-start">
			<nav class="vector-main-menu-landmark" aria-label="Site" role="navigation">
				
<div id="vector-main-menu-dropdown" class="vector-dropdown vector-main-menu-dropdown vector-button-flush-left vector-button-flush-right"  >
	<input type="checkbox" id="vector-main-menu-dropdown-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-main-menu-dropdown" class="vector-dropdown-checkbox "  aria-label="Main menu"  >
	<label id="vector-main-menu-dropdown-label" for="vector-main-menu-dropdown-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only " aria-hidden="true"  ><span class="vector-icon mw-ui-icon-menu mw-ui-icon-wikimedia-menu"></span>

<span class="vector-dropdown-label-text">Main menu</span>
	</label>
	<div class="vector-dropdown-content">


				<div id="vector-main-menu-unpinned-container" class="vector-unpinned-container">
		
<div id="vector-main-menu" class="vector-main-menu vector-pinnable-element">
	<div
	class="vector-pinnable-header vector-main-menu-pinnable-header vector-pinnable-header-unpinned"
	data-feature-name="main-menu-pinned"
	data-pinnable-element-id="vector-main-menu"
	data-pinned-container-id="vector-main-menu-pinned-container"
	data-unpinned-container-id="vector-main-menu-unpinned-container"
>
	<div class="vector-pinnable-header-label">Main menu</div>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-pin-button" data-event-name="pinnable-header.vector-main-menu.pin">move to sidebar</button>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-unpin-button" data-event-name="pinnable-header.vector-main-menu.unpin">hide</button>
</div>

	
<div id="p-navigation" class="vector-menu mw-portlet mw-portlet-navigation"  >
	<div class="vector-menu-heading">
		Navigation
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="n-mainpage-description" class="mw-list-item"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z"><span>Main page</span></a></li><li id="n-contents" class="mw-list-item"><a href="/wiki/Wikipedia:Contents" title="Guides to browsing Wikipedia"><span>Contents</span></a></li><li id="n-currentevents" class="mw-list-item"><a href="/wiki/Portal:Current_events" title="Articles related to current events"><span>Current events</span></a></li><li id="n-randompage" class="mw-list-item"><a href="/wiki/Special:Random" title="Visit a randomly selected article [x]" accesskey="x"><span>Random article</span></a></li><li id="n-aboutsite" class="mw-list-item"><a href="/wiki/Wikipedia:About" title="Learn about Wikipedia and how it works"><span>About Wikipedia</span></a></li><li id="n-contactpage" class="mw-list-item"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia"><span>Contact us</span></a></li><li id="n-sitesupport" class="mw-list-item"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en" title="Support us by donating to the Wikimedia Foundation"><span>Donate</span></a></li>
		</ul>
		
	</div>
</div>

	
	
<div id="p-interaction" class="vector-menu mw-portlet mw-portlet-interaction"  >
	<div class="vector-menu-heading">
		Contribute
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="n-help" class="mw-list-item"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia"><span>Help</span></a></li><li id="n-introduction" class="mw-list-item"><a href="/wiki/Help:Introduction" title="Learn how to edit Wikipedia"><span>Learn to edit</span></a></li><li id="n-portal" class="mw-list-item"><a href="/wiki/Wikipedia:Community_portal" title="The hub for editors"><span>Community portal</span></a></li><li id="n-recentchanges" class="mw-list-item"><a href="/wiki/Special:RecentChanges" title="A list of recent changes to Wikipedia [r]" accesskey="r"><span>Recent changes</span></a></li><li id="n-upload" class="mw-list-item"><a href="/wiki/Wikipedia:File_upload_wizard" title="Add images or other media for use on Wikipedia"><span>Upload file</span></a></li>
		</ul>
		
	</div>
</div>

</div>

				</div>

	</div>
</div>

		</nav>
			
<a href="/wiki/Main_Page" class="mw-logo">
	<img class="mw-logo-icon" src="/static/images/icons/wikipedia.png" alt="" aria-hidden="true" height="50" width="50">
	<span class="mw-logo-container">
		<img class="mw-logo-wordmark" alt="Wikipedia" src="/static/images/mobile/copyright/wikipedia-wordmark-en.svg" style="width: 7.5em; height: 1.125em;">
		<img class="mw-logo-tagline" alt="The Free Encyclopedia" src="/static/images/mobile/copyright/wikipedia-tagline-en.svg" width="117" height="13" style="width: 7.3125em; height: 0.8125em;">
	</span>
</a>

		</div>
		<div class="vector-header-end">
			
<div id="p-search" role="search" class="vector-search-box-vue  vector-search-box-collapses vector-search-box-show-thumbnail vector-search-box-auto-expand-width vector-search-box">
	<a href="/wiki/Special:Search" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only search-toggle" id="" title="Search Wikipedia [f]" accesskey="f"><span class="vector-icon mw-ui-icon-search mw-ui-icon-wikimedia-search"></span>

<span>Search</span>
	</a>
	<div class="vector-typeahead-search-container">
		<div class="cdx-typeahead-search cdx-typeahead-search--show-thumbnail cdx-typeahead-search--auto-expand-width">
			<form action="/w/index.php" id="searchform" class="cdx-search-input cdx-search-input--has-end-button">
				<div id="simpleSearch" class="cdx-search-input__input-wrapper"  data-search-loc="header-moved">
					<div class="cdx-text-input cdx-text-input--has-start-icon">
						<input
							class="cdx-text-input__input"
							 type="search" name="search" placeholder="Search Wikipedia" aria-label="Search Wikipedia" autocapitalize="sentences" title="Search Wikipedia [f]" accesskey="f" id="searchInput"
							>
						<span class="cdx-text-input__icon cdx-text-input__start-icon"></span>
					</div>
					<input type="hidden" name="title" value="Special:Search">
				</div>
				<button class="cdx-button cdx-search-input__end-button">Search</button>
			</form>
		</div>
	</div>
</div>

			<nav class="vector-user-links vector-user-links-wide" aria-label="Personal tools" role="navigation" >
	<div class="vector-user-links-main">
	
<div id="p-vector-user-menu-preferences" class="vector-menu mw-portlet emptyPortlet"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			
		</ul>
		
	</div>
</div>

	
<div id="p-vector-user-menu-userpage" class="vector-menu mw-portlet emptyPortlet"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			
		</ul>
		
	</div>
</div>

	<nav class="vector-client-prefs-landmark" aria-label="Appearance">
		
		
	</nav>
	
<div id="p-vector-user-menu-notifications" class="vector-menu mw-portlet emptyPortlet"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			
		</ul>
		
	</div>
</div>

	
<div id="p-vector-user-menu-overflow" class="vector-menu mw-portlet"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			<li id="pt-createaccount-2" class="user-links-collapsible-item mw-list-item user-links-collapsible-item"><a data-mw="interface" href="/w/index.php?title=Special:CreateAccount&amp;returnto=Word2vec" title="You are encouraged to create an account and log in; however, it is not mandatory" class=""><span>Create account</span></a>
</li>
<li id="pt-login-2" class="user-links-collapsible-item mw-list-item user-links-collapsible-item"><a data-mw="interface" href="/w/index.php?title=Special:UserLogin&amp;returnto=Word2vec" title="You&#039;re encouraged to log in; however, it&#039;s not mandatory. [o]" accesskey="o" class=""><span>Log in</span></a>
</li>

			
		</ul>
		
	</div>
</div>

	</div>
	
<div id="vector-user-links-dropdown" class="vector-dropdown vector-user-menu vector-button-flush-right vector-user-menu-logged-out"  title="Log in and more options" >
	<input type="checkbox" id="vector-user-links-dropdown-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-user-links-dropdown" class="vector-dropdown-checkbox "  aria-label="Personal tools"  >
	<label id="vector-user-links-dropdown-label" for="vector-user-links-dropdown-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only " aria-hidden="true"  ><span class="vector-icon mw-ui-icon-ellipsis mw-ui-icon-wikimedia-ellipsis"></span>

<span class="vector-dropdown-label-text">Personal tools</span>
	</label>
	<div class="vector-dropdown-content">


		
<div id="p-personal" class="vector-menu mw-portlet mw-portlet-personal user-links-collapsible-item"  title="User menu" >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="pt-createaccount" class="user-links-collapsible-item mw-list-item"><a href="/w/index.php?title=Special:CreateAccount&amp;returnto=Word2vec" title="You are encouraged to create an account and log in; however, it is not mandatory"><span class="vector-icon mw-ui-icon-userAdd mw-ui-icon-wikimedia-userAdd"></span> <span>Create account</span></a></li><li id="pt-login" class="user-links-collapsible-item mw-list-item"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Word2vec" title="You&#039;re encouraged to log in; however, it&#039;s not mandatory. [o]" accesskey="o"><span class="vector-icon mw-ui-icon-logIn mw-ui-icon-wikimedia-logIn"></span> <span>Log in</span></a></li>
		</ul>
		
	</div>
</div>

<div id="p-user-menu-anon-editor" class="vector-menu mw-portlet mw-portlet-user-menu-anon-editor"  >
	<div class="vector-menu-heading">
		Pages for logged out editors <a href="/wiki/Help:Introduction" aria-label="Learn more about editing"><span>learn more</span></a>
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="pt-anoncontribs" class="mw-list-item"><a href="/wiki/Special:MyContributions" title="A list of edits made from this IP address [y]" accesskey="y"><span>Contributions</span></a></li><li id="pt-anontalk" class="mw-list-item"><a href="/wiki/Special:MyTalk" title="Discussion about edits from this IP address [n]" accesskey="n"><span>Talk</span></a></li>
		</ul>
		
	</div>
</div>

	
	</div>
</div>

</nav>

		</div>
	</header>
</div>
<div class="mw-page-container">
	<div class="mw-page-container-inner">
		<div class="vector-sitenotice-container">
			<div id="siteNotice"><!-- CentralNotice --></div>
		</div>
		<div class="vector-column-start">
			<div class="vector-main-menu-container">
		<div id="mw-navigation">
			<nav id="mw-panel" class="vector-main-menu-landmark" aria-label="Site" role="navigation">
				<div id="vector-main-menu-pinned-container" class="vector-pinned-container">
				
				</div>
		</nav>
		</div>
	</div>
	<div class="vector-sticky-pinned-container">
				<nav id="mw-panel-toc" role="navigation" aria-label="Contents" data-event-name="ui.sidebar-toc" class="mw-table-of-contents-container vector-toc-landmark">
					<div id="vector-toc-pinned-container" class="vector-pinned-container">
					<div id="vector-toc" class="vector-toc vector-pinnable-element">
	<div
	class="vector-pinnable-header vector-toc-pinnable-header vector-pinnable-header-pinned"
	data-feature-name="toc-pinned"
	data-pinnable-element-id="vector-toc"
	
	
>
	<h2 class="vector-pinnable-header-label">Contents</h2>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-pin-button" data-event-name="pinnable-header.vector-toc.pin">move to sidebar</button>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-unpin-button" data-event-name="pinnable-header.vector-toc.unpin">hide</button>
</div>


	<ul class="vector-toc-contents" id="mw-panel-toc-list">
		<li id="toc-mw-content-text"
			class="vector-toc-list-item vector-toc-level-1">
			<a href="#" class="vector-toc-link">
				<div class="vector-toc-text">(Top)</div>
			</a>
		</li>
		<li id="toc-Approach"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Approach">
			<div class="vector-toc-text">
			<span class="vector-toc-numb">1</span>Approach</div>
		</a>
		
		<ul id="toc-Approach-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-Mathematical_details"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Mathematical_details">
			<div class="vector-toc-text">
			<span class="vector-toc-numb">2</span>Mathematical details</div>
		</a>
		
			<button aria-controls="toc-Mathematical_details-sublist" class="cdx-button cdx-button--weight-quiet cdx-button--icon-only vector-toc-toggle">
				<span class="vector-icon vector-icon--x-small mw-ui-icon-wikimedia-expand"></span>
				<span>Toggle Mathematical details subsection</span>
			</button>
		
		<ul id="toc-Mathematical_details-sublist" class="vector-toc-list">
			<li id="toc-Continuous_Bag_of_Words_(CBOW)"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Continuous_Bag_of_Words_(CBOW)">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">2.1</span>Continuous Bag of Words (CBOW)</div>
			</a>
			
			<ul id="toc-Continuous_Bag_of_Words_(CBOW)-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Skip-gram"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Skip-gram">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">2.2</span>Skip-gram</div>
			</a>
			
			<ul id="toc-Skip-gram-sublist" class="vector-toc-list">
			</ul>
		</li>
	</ul>
	</li>
	<li id="toc-History"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#History">
			<div class="vector-toc-text">
			<span class="vector-toc-numb">3</span>History</div>
		</a>
		
		<ul id="toc-History-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-Parameterization"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Parameterization">
			<div class="vector-toc-text">
			<span class="vector-toc-numb">4</span>Parameterization</div>
		</a>
		
			<button aria-controls="toc-Parameterization-sublist" class="cdx-button cdx-button--weight-quiet cdx-button--icon-only vector-toc-toggle">
				<span class="vector-icon vector-icon--x-small mw-ui-icon-wikimedia-expand"></span>
				<span>Toggle Parameterization subsection</span>
			</button>
		
		<ul id="toc-Parameterization-sublist" class="vector-toc-list">
			<li id="toc-Training_algorithm"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Training_algorithm">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">4.1</span>Training algorithm</div>
			</a>
			
			<ul id="toc-Training_algorithm-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Sub-sampling"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Sub-sampling">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">4.2</span>Sub-sampling</div>
			</a>
			
			<ul id="toc-Sub-sampling-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Dimensionality"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Dimensionality">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">4.3</span>Dimensionality</div>
			</a>
			
			<ul id="toc-Dimensionality-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Context_window"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Context_window">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">4.4</span>Context window</div>
			</a>
			
			<ul id="toc-Context_window-sublist" class="vector-toc-list">
			</ul>
		</li>
	</ul>
	</li>
	<li id="toc-Extensions"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Extensions">
			<div class="vector-toc-text">
			<span class="vector-toc-numb">5</span>Extensions</div>
		</a>
		
			<button aria-controls="toc-Extensions-sublist" class="cdx-button cdx-button--weight-quiet cdx-button--icon-only vector-toc-toggle">
				<span class="vector-icon vector-icon--x-small mw-ui-icon-wikimedia-expand"></span>
				<span>Toggle Extensions subsection</span>
			</button>
		
		<ul id="toc-Extensions-sublist" class="vector-toc-list">
			<li id="toc-doc2vec"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#doc2vec">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">5.1</span>doc2vec</div>
			</a>
			
			<ul id="toc-doc2vec-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-top2vec"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#top2vec">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">5.2</span>top2vec</div>
			</a>
			
			<ul id="toc-top2vec-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-BioVectors"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#BioVectors">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">5.3</span>BioVectors</div>
			</a>
			
			<ul id="toc-BioVectors-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Radiology_and_intelligent_word_embeddings_(IWE)"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Radiology_and_intelligent_word_embeddings_(IWE)">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">5.4</span>Radiology and intelligent word embeddings (IWE)</div>
			</a>
			
			<ul id="toc-Radiology_and_intelligent_word_embeddings_(IWE)-sublist" class="vector-toc-list">
			</ul>
		</li>
	</ul>
	</li>
	<li id="toc-Analysis"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Analysis">
			<div class="vector-toc-text">
			<span class="vector-toc-numb">6</span>Analysis</div>
		</a>
		
		<ul id="toc-Analysis-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-Preservation_of_semantic_and_syntactic_relationships"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Preservation_of_semantic_and_syntactic_relationships">
			<div class="vector-toc-text">
			<span class="vector-toc-numb">7</span>Preservation of semantic and syntactic relationships</div>
		</a>
		
		<ul id="toc-Preservation_of_semantic_and_syntactic_relationships-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-Assessing_the_quality_of_a_model"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Assessing_the_quality_of_a_model">
			<div class="vector-toc-text">
			<span class="vector-toc-numb">8</span>Assessing the quality of a model</div>
		</a>
		
			<button aria-controls="toc-Assessing_the_quality_of_a_model-sublist" class="cdx-button cdx-button--weight-quiet cdx-button--icon-only vector-toc-toggle">
				<span class="vector-icon vector-icon--x-small mw-ui-icon-wikimedia-expand"></span>
				<span>Toggle Assessing the quality of a model subsection</span>
			</button>
		
		<ul id="toc-Assessing_the_quality_of_a_model-sublist" class="vector-toc-list">
			<li id="toc-Parameters_and_model_quality"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Parameters_and_model_quality">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">8.1</span>Parameters and model quality</div>
			</a>
			
			<ul id="toc-Parameters_and_model_quality-sublist" class="vector-toc-list">
			</ul>
		</li>
	</ul>
	</li>
	<li id="toc-See_also"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#See_also">
			<div class="vector-toc-text">
			<span class="vector-toc-numb">9</span>See also</div>
		</a>
		
		<ul id="toc-See_also-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-References"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#References">
			<div class="vector-toc-text">
			<span class="vector-toc-numb">10</span>References</div>
		</a>
		
		<ul id="toc-References-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-External_links"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#External_links">
			<div class="vector-toc-text">
			<span class="vector-toc-numb">11</span>External links</div>
		</a>
		
			<button aria-controls="toc-External_links-sublist" class="cdx-button cdx-button--weight-quiet cdx-button--icon-only vector-toc-toggle">
				<span class="vector-icon vector-icon--x-small mw-ui-icon-wikimedia-expand"></span>
				<span>Toggle External links subsection</span>
			</button>
		
		<ul id="toc-External_links-sublist" class="vector-toc-list">
			<li id="toc-Implementations"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Implementations">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">11.1</span>Implementations</div>
			</a>
			
			<ul id="toc-Implementations-sublist" class="vector-toc-list">
			</ul>
		</li>
	</ul>
	</li>
</ul>
</div>

					</div>
		</nav>
			</div>
		</div>
		<div class="mw-content-container">
			<main id="content" class="mw-body" role="main">
				<header class="mw-body-header vector-page-titlebar">
					<nav role="navigation" aria-label="Contents" class="vector-toc-landmark">
						
<div id="vector-page-titlebar-toc" class="vector-dropdown vector-page-titlebar-toc vector-button-flush-left"  >
	<input type="checkbox" id="vector-page-titlebar-toc-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-page-titlebar-toc" class="vector-dropdown-checkbox "  aria-label="Toggle the table of contents"  >
	<label id="vector-page-titlebar-toc-label" for="vector-page-titlebar-toc-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only " aria-hidden="true"  ><span class="vector-icon mw-ui-icon-listBullet mw-ui-icon-wikimedia-listBullet"></span>

<span class="vector-dropdown-label-text">Toggle the table of contents</span>
	</label>
	<div class="vector-dropdown-content">


							<div id="vector-page-titlebar-toc-unpinned-container" class="vector-unpinned-container">
			</div>
		
	</div>
</div>

					</nav>
					<h1 id="firstHeading" class="firstHeading mw-first-heading"><span class="mw-page-title-main">Word2vec</span></h1>
							
<div id="p-lang-btn" class="vector-dropdown mw-portlet mw-portlet-lang"  >
	<input type="checkbox" id="p-lang-btn-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-p-lang-btn" class="vector-dropdown-checkbox mw-interlanguage-selector" aria-label="Go to an article in another language. Available in 17 languages"   >
	<label id="p-lang-btn-label" for="p-lang-btn-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--action-progressive mw-portlet-lang-heading-17" aria-hidden="true"  ><span class="vector-icon mw-ui-icon-language-progressive mw-ui-icon-wikimedia-language-progressive"></span>

<span class="vector-dropdown-label-text">17 languages</span>
	</label>
	<div class="vector-dropdown-content">

		<div class="vector-menu-content">
			
			<ul class="vector-menu-content-list">
				
				<li class="interlanguage-link interwiki-ar mw-list-item"><a href="https://ar.wikipedia.org/wiki/%D8%AA%D8%B6%D9%85%D9%8A%D9%86_%D8%A7%D9%84%D8%A3%D8%B4%D8%B9%D8%A9_%D8%A7%D9%84%D9%83%D9%84%D9%85%D8%A7%D8%AA" title="تضمين الأشعة الكلمات – Arabic" lang="ar" hreflang="ar" class="interlanguage-link-target"><span>العربية</span></a></li><li class="interlanguage-link interwiki-bn mw-list-item"><a href="https://bn.wikipedia.org/wiki/%E0%A6%93%E0%A6%AF%E0%A6%BC%E0%A6%BE%E0%A6%B0%E0%A7%8D%E0%A6%A1%E0%A6%9F%E0%A7%81%E0%A6%AD%E0%A7%87%E0%A6%95" title="ওয়ার্ডটুভেক – Bangla" lang="bn" hreflang="bn" class="interlanguage-link-target"><span>বাংলা</span></a></li><li class="interlanguage-link interwiki-ca mw-list-item"><a href="https://ca.wikipedia.org/wiki/Word2vec" title="Word2vec – Catalan" lang="ca" hreflang="ca" class="interlanguage-link-target"><span>Català</span></a></li><li class="interlanguage-link interwiki-cs mw-list-item"><a href="https://cs.wikipedia.org/wiki/Word2Vec" title="Word2Vec – Czech" lang="cs" hreflang="cs" class="interlanguage-link-target"><span>Čeština</span></a></li><li class="interlanguage-link interwiki-es mw-list-item"><a href="https://es.wikipedia.org/wiki/Word2vec" title="Word2vec – Spanish" lang="es" hreflang="es" class="interlanguage-link-target"><span>Español</span></a></li><li class="interlanguage-link interwiki-eu mw-list-item"><a href="https://eu.wikipedia.org/wiki/Word2vec" title="Word2vec – Basque" lang="eu" hreflang="eu" class="interlanguage-link-target"><span>Euskara</span></a></li><li class="interlanguage-link interwiki-fa mw-list-item"><a href="https://fa.wikipedia.org/wiki/Word2vec" title="Word2vec – Persian" lang="fa" hreflang="fa" class="interlanguage-link-target"><span>فارسی</span></a></li><li class="interlanguage-link interwiki-fr mw-list-item"><a href="https://fr.wikipedia.org/wiki/Word2vec" title="Word2vec – French" lang="fr" hreflang="fr" class="interlanguage-link-target"><span>Français</span></a></li><li class="interlanguage-link interwiki-ko mw-list-item"><a href="https://ko.wikipedia.org/wiki/Word2vec" title="Word2vec – Korean" lang="ko" hreflang="ko" class="interlanguage-link-target"><span>한국어</span></a></li><li class="interlanguage-link interwiki-it mw-list-item"><a href="https://it.wikipedia.org/wiki/Word2vec" title="Word2vec – Italian" lang="it" hreflang="it" class="interlanguage-link-target"><span>Italiano</span></a></li><li class="interlanguage-link interwiki-he mw-list-item"><a href="https://he.wikipedia.org/wiki/Word2vec" title="Word2vec – Hebrew" lang="he" hreflang="he" class="interlanguage-link-target"><span>עברית</span></a></li><li class="interlanguage-link interwiki-ja mw-list-item"><a href="https://ja.wikipedia.org/wiki/Word2vec" title="Word2vec – Japanese" lang="ja" hreflang="ja" class="interlanguage-link-target"><span>日本語</span></a></li><li class="interlanguage-link interwiki-ru mw-list-item"><a href="https://ru.wikipedia.org/wiki/Word2vec" title="Word2vec – Russian" lang="ru" hreflang="ru" class="interlanguage-link-target"><span>Русский</span></a></li><li class="interlanguage-link interwiki-uk mw-list-item"><a href="https://uk.wikipedia.org/wiki/Word2vec" title="Word2vec – Ukrainian" lang="uk" hreflang="uk" class="interlanguage-link-target"><span>Українська</span></a></li><li class="interlanguage-link interwiki-vi mw-list-item"><a href="https://vi.wikipedia.org/wiki/Word2vec" title="Word2vec – Vietnamese" lang="vi" hreflang="vi" class="interlanguage-link-target"><span>Tiếng Việt</span></a></li><li class="interlanguage-link interwiki-zh-yue mw-list-item"><a href="https://zh-yue.wikipedia.org/wiki/Word2vec" title="Word2vec – Cantonese" lang="yue" hreflang="yue" class="interlanguage-link-target"><span>粵語</span></a></li><li class="interlanguage-link interwiki-zh mw-list-item"><a href="https://zh.wikipedia.org/wiki/Word2vec" title="Word2vec – Chinese" lang="zh" hreflang="zh" class="interlanguage-link-target"><span>中文</span></a></li>
			</ul>
			<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q22673982#sitelinks-wikipedia" title="Edit interlanguage links" class="wbc-editpage">Edit links</a></span></div>
		</div>

	</div>
</div>
</header>
				<div class="vector-page-toolbar">
					<div class="vector-page-toolbar-container">
						<div id="left-navigation">
							<nav aria-label="Namespaces">
								
<div id="p-associated-pages" class="vector-menu vector-menu-tabs mw-portlet mw-portlet-associated-pages"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="ca-nstab-main" class="selected vector-tab-noicon mw-list-item"><a href="/wiki/Word2vec" title="View the content page [c]" accesskey="c"><span>Article</span></a></li><li id="ca-talk" class="vector-tab-noicon mw-list-item"><a href="/wiki/Talk:Word2vec" rel="discussion" title="Discuss improvements to the content page [t]" accesskey="t"><span>Talk</span></a></li>
		</ul>
		
	</div>
</div>

								
<div id="p-variants" class="vector-dropdown emptyPortlet"  >
	<input type="checkbox" id="p-variants-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-p-variants" class="vector-dropdown-checkbox " aria-label="Change language variant"   >
	<label id="p-variants-label" for="p-variants-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet" aria-hidden="true"  ><span class="vector-dropdown-label-text">English</span>
	</label>
	<div class="vector-dropdown-content">


					
<div id="p-variants" class="vector-menu mw-portlet mw-portlet-variants emptyPortlet"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			
		</ul>
		
	</div>
</div>

				
	</div>
</div>

							</nav>
						</div>
						<div id="right-navigation" class="vector-collapsible">
							<nav aria-label="Views">
								
<div id="p-views" class="vector-menu vector-menu-tabs mw-portlet mw-portlet-views"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="ca-view" class="selected vector-tab-noicon mw-list-item"><a href="/wiki/Word2vec"><span>Read</span></a></li><li id="ca-edit" class="vector-tab-noicon mw-list-item"><a href="/w/index.php?title=Word2vec&amp;action=edit" title="Edit this page [e]" accesskey="e"><span>Edit</span></a></li><li id="ca-history" class="vector-tab-noicon mw-list-item"><a href="/w/index.php?title=Word2vec&amp;action=history" title="Past revisions of this page [h]" accesskey="h"><span>View history</span></a></li>
		</ul>
		
	</div>
</div>

							</nav>
				
							<nav class="vector-page-tools-landmark" aria-label="Page tools">
								
<div id="vector-page-tools-dropdown" class="vector-dropdown vector-page-tools-dropdown"  >
	<input type="checkbox" id="vector-page-tools-dropdown-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-page-tools-dropdown" class="vector-dropdown-checkbox "  aria-label="Tools"  >
	<label id="vector-page-tools-dropdown-label" for="vector-page-tools-dropdown-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet" aria-hidden="true"  ><span class="vector-dropdown-label-text">Tools</span>
	</label>
	<div class="vector-dropdown-content">


									<div id="vector-page-tools-unpinned-container" class="vector-unpinned-container">
						
<div id="vector-page-tools" class="vector-page-tools vector-pinnable-element">
	<div
	class="vector-pinnable-header vector-page-tools-pinnable-header vector-pinnable-header-unpinned"
	data-feature-name="page-tools-pinned"
	data-pinnable-element-id="vector-page-tools"
	data-pinned-container-id="vector-page-tools-pinned-container"
	data-unpinned-container-id="vector-page-tools-unpinned-container"
>
	<div class="vector-pinnable-header-label">Tools</div>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-pin-button" data-event-name="pinnable-header.vector-page-tools.pin">move to sidebar</button>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-unpin-button" data-event-name="pinnable-header.vector-page-tools.unpin">hide</button>
</div>

	
<div id="p-cactions" class="vector-menu mw-portlet mw-portlet-cactions emptyPortlet vector-has-collapsible-items"  title="More options" >
	<div class="vector-menu-heading">
		Actions
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="ca-more-view" class="selected vector-more-collapsible-item mw-list-item"><a href="/wiki/Word2vec"><span>Read</span></a></li><li id="ca-more-edit" class="vector-more-collapsible-item mw-list-item"><a href="/w/index.php?title=Word2vec&amp;action=edit" title="Edit this page [e]" accesskey="e"><span>Edit</span></a></li><li id="ca-more-history" class="vector-more-collapsible-item mw-list-item"><a href="/w/index.php?title=Word2vec&amp;action=history"><span>View history</span></a></li>
		</ul>
		
	</div>
</div>

<div id="p-tb" class="vector-menu mw-portlet mw-portlet-tb"  >
	<div class="vector-menu-heading">
		General
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="t-whatlinkshere" class="mw-list-item"><a href="/wiki/Special:WhatLinksHere/Word2vec" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j"><span>What links here</span></a></li><li id="t-recentchangeslinked" class="mw-list-item"><a href="/wiki/Special:RecentChangesLinked/Word2vec" rel="nofollow" title="Recent changes in pages linked from this page [k]" accesskey="k"><span>Related changes</span></a></li><li id="t-upload" class="mw-list-item"><a href="/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]" accesskey="u"><span>Upload file</span></a></li><li id="t-specialpages" class="mw-list-item"><a href="/wiki/Special:SpecialPages" title="A list of all special pages [q]" accesskey="q"><span>Special pages</span></a></li><li id="t-permalink" class="mw-list-item"><a href="/w/index.php?title=Word2vec&amp;oldid=1215655449" title="Permanent link to this revision of this page"><span>Permanent link</span></a></li><li id="t-info" class="mw-list-item"><a href="/w/index.php?title=Word2vec&amp;action=info" title="More information about this page"><span>Page information</span></a></li><li id="t-cite" class="mw-list-item"><a href="/w/index.php?title=Special:CiteThisPage&amp;page=Word2vec&amp;id=1215655449&amp;wpFormIdentifier=titleform" title="Information on how to cite this page"><span>Cite this page</span></a></li><li id="t-urlshortener" class="mw-list-item"><a href="/w/index.php?title=Special:UrlShortener&amp;url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FWord2vec"><span>Get shortened URL</span></a></li><li id="t-urlshortener-qrcode" class="mw-list-item"><a href="/w/index.php?title=Special:QrCode&amp;url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FWord2vec"><span>Download QR code</span></a></li><li id="t-wikibase" class="mw-list-item"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q22673982" title="Structured data on this page hosted by Wikidata [g]" accesskey="g"><span>Wikidata item</span></a></li>
		</ul>
		
	</div>
</div>

<div id="p-coll-print_export" class="vector-menu mw-portlet mw-portlet-coll-print_export"  >
	<div class="vector-menu-heading">
		Print/export
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="coll-download-as-rl" class="mw-list-item"><a href="/w/index.php?title=Special:DownloadAsPdf&amp;page=Word2vec&amp;action=show-download-screen" title="Download this page as a PDF file"><span>Download as PDF</span></a></li><li id="t-print" class="mw-list-item"><a href="/w/index.php?title=Word2vec&amp;printable=yes" title="Printable version of this page [p]" accesskey="p"><span>Printable version</span></a></li>
		</ul>
		
	</div>
</div>

</div>

									</div>
				
	</div>
</div>

							</nav>
						</div>
					</div>
				</div>
				<div class="vector-column-end">
					<div class="vector-sticky-pinned-container">
						<nav class="vector-page-tools-landmark" aria-label="Page tools">
							<div id="vector-page-tools-pinned-container" class="vector-pinned-container">
				
							</div>
		</nav>
						<nav class="vector-client-prefs-landmark" aria-label="Appearance">
						</nav>
					</div>
				</div>
				<div id="bodyContent" class="vector-body" aria-labelledby="firstHeading" data-mw-ve-target-container>
					<div class="vector-body-before-content">
							<div class="mw-indicators">
		</div>

						<div id="siteSub" class="noprint">From Wikipedia, the free encyclopedia</div>
					</div>
					<div id="contentSub"><div id="mw-content-subtitle"></div></div>
					
					
					<div id="mw-content-text" class="mw-body-content"><div class="mw-content-ltr mw-parser-output" lang="en" dir="ltr"><div class="shortdescription nomobile noexcerpt noprint searchaux" style="display:none">Models used to produce word embeddings</div>
<style data-mw-deduplicate="TemplateStyles:r1129693374">.mw-parser-output .hlist dl,.mw-parser-output .hlist ol,.mw-parser-output .hlist ul{margin:0;padding:0}.mw-parser-output .hlist dd,.mw-parser-output .hlist dt,.mw-parser-output .hlist li{margin:0;display:inline}.mw-parser-output .hlist.inline,.mw-parser-output .hlist.inline dl,.mw-parser-output .hlist.inline ol,.mw-parser-output .hlist.inline ul,.mw-parser-output .hlist dl dl,.mw-parser-output .hlist dl ol,.mw-parser-output .hlist dl ul,.mw-parser-output .hlist ol dl,.mw-parser-output .hlist ol ol,.mw-parser-output .hlist ol ul,.mw-parser-output .hlist ul dl,.mw-parser-output .hlist ul ol,.mw-parser-output .hlist ul ul{display:inline}.mw-parser-output .hlist .mw-empty-li{display:none}.mw-parser-output .hlist dt::after{content:": "}.mw-parser-output .hlist dd::after,.mw-parser-output .hlist li::after{content:" · ";font-weight:bold}.mw-parser-output .hlist dd:last-child::after,.mw-parser-output .hlist dt:last-child::after,.mw-parser-output .hlist li:last-child::after{content:none}.mw-parser-output .hlist dd dd:first-child::before,.mw-parser-output .hlist dd dt:first-child::before,.mw-parser-output .hlist dd li:first-child::before,.mw-parser-output .hlist dt dd:first-child::before,.mw-parser-output .hlist dt dt:first-child::before,.mw-parser-output .hlist dt li:first-child::before,.mw-parser-output .hlist li dd:first-child::before,.mw-parser-output .hlist li dt:first-child::before,.mw-parser-output .hlist li li:first-child::before{content:" (";font-weight:normal}.mw-parser-output .hlist dd dd:last-child::after,.mw-parser-output .hlist dd dt:last-child::after,.mw-parser-output .hlist dd li:last-child::after,.mw-parser-output .hlist dt dd:last-child::after,.mw-parser-output .hlist dt dt:last-child::after,.mw-parser-output .hlist dt li:last-child::after,.mw-parser-output .hlist li dd:last-child::after,.mw-parser-output .hlist li dt:last-child::after,.mw-parser-output .hlist li li:last-child::after{content:")";font-weight:normal}.mw-parser-output .hlist ol{counter-reset:listitem}.mw-parser-output .hlist ol>li{counter-increment:listitem}.mw-parser-output .hlist ol>li::before{content:" "counter(listitem)"\a0 "}.mw-parser-output .hlist dd ol>li:first-child::before,.mw-parser-output .hlist dt ol>li:first-child::before,.mw-parser-output .hlist li ol>li:first-child::before{content:" ("counter(listitem)"\a0 "}</style><style data-mw-deduplicate="TemplateStyles:r1045330069">.mw-parser-output .sidebar{width:22em;float:right;clear:right;margin:0.5em 0 1em 1em;background:#f8f9fa;border:1px solid #aaa;padding:0.2em;text-align:center;line-height:1.4em;font-size:88%;border-collapse:collapse;display:table}body.skin-minerva .mw-parser-output .sidebar{display:table!important;float:right!important;margin:0.5em 0 1em 1em!important}.mw-parser-output .sidebar-subgroup{width:100%;margin:0;border-spacing:0}.mw-parser-output .sidebar-left{float:left;clear:left;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-none{float:none;clear:both;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-outer-title{padding:0 0.4em 0.2em;font-size:125%;line-height:1.2em;font-weight:bold}.mw-parser-output .sidebar-top-image{padding:0.4em}.mw-parser-output .sidebar-top-caption,.mw-parser-output .sidebar-pretitle-with-top-image,.mw-parser-output .sidebar-caption{padding:0.2em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-pretitle{padding:0.4em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-title,.mw-parser-output .sidebar-title-with-pretitle{padding:0.2em 0.8em;font-size:145%;line-height:1.2em}.mw-parser-output .sidebar-title-with-pretitle{padding:0.1em 0.4em}.mw-parser-output .sidebar-image{padding:0.2em 0.4em 0.4em}.mw-parser-output .sidebar-heading{padding:0.1em 0.4em}.mw-parser-output .sidebar-content{padding:0 0.5em 0.4em}.mw-parser-output .sidebar-content-with-subgroup{padding:0.1em 0.4em 0.2em}.mw-parser-output .sidebar-above,.mw-parser-output .sidebar-below{padding:0.3em 0.8em;font-weight:bold}.mw-parser-output .sidebar-collapse .sidebar-above,.mw-parser-output .sidebar-collapse .sidebar-below{border-top:1px solid #aaa;border-bottom:1px solid #aaa}.mw-parser-output .sidebar-navbar{text-align:right;font-size:115%;padding:0 0.4em 0.4em}.mw-parser-output .sidebar-list-title{padding:0 0.4em;text-align:left;font-weight:bold;line-height:1.6em;font-size:105%}.mw-parser-output .sidebar-list-title-c{padding:0 0.4em;text-align:center;margin:0 3.3em}@media(max-width:720px){body.mediawiki .mw-parser-output .sidebar{width:100%!important;clear:both;float:none!important;margin-left:0!important;margin-right:0!important}}</style><style data-mw-deduplicate="TemplateStyles:r886047488">.mw-parser-output .nobold{font-weight:normal}</style><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886047488"><table class="sidebar sidebar-collapse nomobile nowraplinks"><tbody><tr><td class="sidebar-pretitle">Part of a series on</td></tr><tr><th class="sidebar-title-with-pretitle"><a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a><br />and <a href="/wiki/Data_mining" title="Data mining">data mining</a></th></tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="border-top:1px solid #aaa;text-align:center; background:#ddd;">Paradigms</div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Supervised_learning" title="Supervised learning">Supervised learning</a></li>
<li><a href="/wiki/Unsupervised_learning" title="Unsupervised learning">Unsupervised learning</a></li>
<li><a href="/wiki/Online_machine_learning" title="Online machine learning">Online learning</a></li>
<li><a href="/wiki/Batch_learning" class="mw-redirect" title="Batch learning">Batch learning</a></li>
<li><a href="/wiki/Meta-learning_(computer_science)" title="Meta-learning (computer science)">Meta-learning</a></li>
<li><a href="/wiki/Semi-supervised_learning" class="mw-redirect" title="Semi-supervised learning">Semi-supervised learning</a></li>
<li><a href="/wiki/Self-supervised_learning" title="Self-supervised learning">Self-supervised learning</a></li>
<li><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></li>
<li><a href="/wiki/Curriculum_learning" title="Curriculum learning">Curriculum learning</a></li>
<li><a href="/wiki/Rule-based_machine_learning" title="Rule-based machine learning">Rule-based learning</a></li>
<li><a href="/wiki/Quantum_machine_learning" title="Quantum machine learning">Quantum machine learning</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="border-top:1px solid #aaa;text-align:center; background:#ddd;">Problems</div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Statistical_classification" title="Statistical classification">Classification</a></li>
<li><a href="/wiki/Generative_model" title="Generative model">Generative modeling</a></li>
<li><a href="/wiki/Regression_analysis" title="Regression analysis">Regression</a></li>
<li><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></li>
<li><a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">Dimensionality reduction</a></li>
<li><a href="/wiki/Density_estimation" title="Density estimation">Density estimation</a></li>
<li><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></li>
<li><a href="/wiki/Data_cleaning" class="mw-redirect" title="Data cleaning">Data cleaning</a></li>
<li><a href="/wiki/Automated_machine_learning" title="Automated machine learning">AutoML</a></li>
<li><a href="/wiki/Association_rule_learning" title="Association rule learning">Association rules</a></li>
<li><a href="/wiki/Semantic_analysis_(machine_learning)" title="Semantic analysis (machine learning)">Semantic analysis</a></li>
<li><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></li>
<li><a href="/wiki/Feature_engineering" title="Feature engineering">Feature engineering</a></li>
<li><a href="/wiki/Feature_learning" title="Feature learning">Feature learning</a></li>
<li><a href="/wiki/Learning_to_rank" title="Learning to rank">Learning to rank</a></li>
<li><a href="/wiki/Grammar_induction" title="Grammar induction">Grammar induction</a></li>
<li><a href="/wiki/Ontology_learning" title="Ontology learning">Ontology learning</a></li>
<li><a href="/wiki/Multimodal_learning" title="Multimodal learning">Multimodal learning</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="border-top:1px solid #aaa;text-align:center; background:#ddd;"><div style="display: inline-block; line-height: 1.2em; padding: .1em 0;"><a href="/wiki/Supervised_learning" title="Supervised learning">Supervised learning</a><br /><span class="nobold"><span style="font-size:85%;">(<b><a href="/wiki/Statistical_classification" title="Statistical classification">classification</a></b>&#160;&#8226;&#32;<b><a href="/wiki/Regression_analysis" title="Regression analysis">regression</a></b>)</span></span> </div></div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Apprenticeship_learning" title="Apprenticeship learning">Apprenticeship learning</a></li>
<li><a href="/wiki/Decision_tree_learning" title="Decision tree learning">Decision trees</a></li>
<li><a href="/wiki/Ensemble_learning" title="Ensemble learning">Ensembles</a>
<ul><li><a href="/wiki/Bootstrap_aggregating" title="Bootstrap aggregating">Bagging</a></li>
<li><a href="/wiki/Boosting_(machine_learning)" title="Boosting (machine learning)">Boosting</a></li>
<li><a href="/wiki/Random_forest" title="Random forest">Random forest</a></li></ul></li>
<li><a href="/wiki/K-nearest_neighbors_algorithm" title="K-nearest neighbors algorithm"><i>k</i>-NN</a></li>
<li><a href="/wiki/Linear_regression" title="Linear regression">Linear regression</a></li>
<li><a href="/wiki/Naive_Bayes_classifier" title="Naive Bayes classifier">Naive Bayes</a></li>
<li><a href="/wiki/Artificial_neural_network" class="mw-redirect" title="Artificial neural network">Artificial neural networks</a></li>
<li><a href="/wiki/Logistic_regression" title="Logistic regression">Logistic regression</a></li>
<li><a href="/wiki/Perceptron" title="Perceptron">Perceptron</a></li>
<li><a href="/wiki/Relevance_vector_machine" title="Relevance vector machine">Relevance vector machine (RVM)</a></li>
<li><a href="/wiki/Support_vector_machine" title="Support vector machine">Support vector machine (SVM)</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="border-top:1px solid #aaa;text-align:center; background:#ddd;"><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/BIRCH" title="BIRCH">BIRCH</a></li>
<li><a href="/wiki/CURE_algorithm" title="CURE algorithm">CURE</a></li>
<li><a href="/wiki/Hierarchical_clustering" title="Hierarchical clustering">Hierarchical</a></li>
<li><a href="/wiki/K-means_clustering" title="K-means clustering"><i>k</i>-means</a></li>
<li><a href="/wiki/Fuzzy_clustering" title="Fuzzy clustering">Fuzzy</a></li>
<li><a href="/wiki/Expectation%E2%80%93maximization_algorithm" title="Expectation–maximization algorithm">Expectation–maximization (EM)</a></li>
<li><br /><a href="/wiki/DBSCAN" title="DBSCAN">DBSCAN</a></li>
<li><a href="/wiki/OPTICS_algorithm" title="OPTICS algorithm">OPTICS</a></li>
<li><a href="/wiki/Mean_shift" title="Mean shift">Mean shift</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="border-top:1px solid #aaa;text-align:center; background:#ddd;"><a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">Dimensionality reduction</a></div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Factor_analysis" title="Factor analysis">Factor analysis</a></li>
<li><a href="/wiki/Canonical_correlation" title="Canonical correlation">CCA</a></li>
<li><a href="/wiki/Independent_component_analysis" title="Independent component analysis">ICA</a></li>
<li><a href="/wiki/Linear_discriminant_analysis" title="Linear discriminant analysis">LDA</a></li>
<li><a href="/wiki/Non-negative_matrix_factorization" title="Non-negative matrix factorization">NMF</a></li>
<li><a href="/wiki/Principal_component_analysis" title="Principal component analysis">PCA</a></li>
<li><a href="/wiki/Proper_generalized_decomposition" title="Proper generalized decomposition">PGD</a></li>
<li><a href="/wiki/T-distributed_stochastic_neighbor_embedding" title="T-distributed stochastic neighbor embedding">t-SNE</a></li>
<li><a href="/wiki/Sparse_dictionary_learning" title="Sparse dictionary learning">SDL</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="border-top:1px solid #aaa;text-align:center; background:#ddd;"><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Graphical_model" title="Graphical model">Graphical models</a>
<ul><li><a href="/wiki/Bayesian_network" title="Bayesian network">Bayes net</a></li>
<li><a href="/wiki/Conditional_random_field" title="Conditional random field">Conditional random field</a></li>
<li><a href="/wiki/Hidden_Markov_model" title="Hidden Markov model">Hidden Markov</a></li></ul></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="border-top:1px solid #aaa;text-align:center; background:#ddd;"><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Random_sample_consensus" title="Random sample consensus">RANSAC</a></li>
<li><a href="/wiki/K-nearest_neighbors_algorithm" title="K-nearest neighbors algorithm"><i>k</i>-NN</a></li>
<li><a href="/wiki/Local_outlier_factor" title="Local outlier factor">Local outlier factor</a></li>
<li><a href="/wiki/Isolation_forest" title="Isolation forest">Isolation forest</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="border-top:1px solid #aaa;text-align:center; background:#ddd;"><a href="/wiki/Artificial_neural_network" class="mw-redirect" title="Artificial neural network">Artificial neural network</a></div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Autoencoder" title="Autoencoder">Autoencoder</a></li>
<li><a href="/wiki/Cognitive_computing" title="Cognitive computing">Cognitive computing</a></li>
<li><a href="/wiki/Deep_learning" title="Deep learning">Deep learning</a></li>
<li><a href="/wiki/DeepDream" title="DeepDream">DeepDream</a></li>
<li><a href="/wiki/Feedforward_neural_network" title="Feedforward neural network">Feedforward neural network</a></li>
<li><a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">Recurrent neural network</a>
<ul><li><a href="/wiki/Long_short-term_memory" title="Long short-term memory">LSTM</a></li>
<li><a href="/wiki/Gated_recurrent_unit" title="Gated recurrent unit">GRU</a></li>
<li><a href="/wiki/Echo_state_network" title="Echo state network">ESN</a></li>
<li><a href="/wiki/Reservoir_computing" title="Reservoir computing">reservoir computing</a></li></ul></li>
<li><a href="/wiki/Restricted_Boltzmann_machine" title="Restricted Boltzmann machine">Restricted Boltzmann machine</a></li>
<li><a href="/wiki/Generative_adversarial_network" title="Generative adversarial network">GAN</a></li>
<li><a href="/wiki/Diffusion_model" title="Diffusion model">Diffusion model</a></li>
<li><a href="/wiki/Self-organizing_map" title="Self-organizing map">SOM</a></li>
<li><a href="/wiki/Convolutional_neural_network" title="Convolutional neural network">Convolutional neural network</a>
<ul><li><a href="/wiki/U-Net" title="U-Net">U-Net</a></li></ul></li>
<li><a href="/wiki/Transformer_(machine_learning_model)" class="mw-redirect" title="Transformer (machine learning model)">Transformer</a>
<ul><li><a href="/wiki/Vision_transformer" title="Vision transformer">Vision</a></li></ul></li>
<li><a href="/wiki/Mamba_(deep_learning_architecture)" title="Mamba (deep learning architecture)">Mamba</a></li>
<li><a href="/wiki/Spiking_neural_network" title="Spiking neural network">Spiking neural network</a></li>
<li><a href="/wiki/Memtransistor" title="Memtransistor">Memtransistor</a></li>
<li><a href="/wiki/Electrochemical_RAM" title="Electrochemical RAM">Electrochemical RAM</a> (ECRAM)</li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="border-top:1px solid #aaa;text-align:center; background:#ddd;"><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Q-learning" title="Q-learning">Q-learning</a></li>
<li><a href="/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action" title="State–action–reward–state–action">SARSA</a></li>
<li><a href="/wiki/Temporal_difference_learning" title="Temporal difference learning">Temporal difference (TD)</a></li>
<li><a href="/wiki/Multi-agent_reinforcement_learning" title="Multi-agent reinforcement learning">Multi-agent</a>
<ul><li><a href="/wiki/Self-play_(reinforcement_learning_technique)" class="mw-redirect" title="Self-play (reinforcement learning technique)">Self-play</a></li></ul></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="border-top:1px solid #aaa;text-align:center; background:#ddd;">Learning with humans</div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Active_learning_(machine_learning)" title="Active learning (machine learning)">Active learning</a></li>
<li><a href="/wiki/Crowdsourcing" title="Crowdsourcing">Crowdsourcing</a></li>
<li><a href="/wiki/Human-in-the-loop" title="Human-in-the-loop">Human-in-the-loop</a></li>
<li><a href="/wiki/Reinforcement_learning_from_human_feedback" title="Reinforcement learning from human feedback">RLHF</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="border-top:1px solid #aaa;text-align:center; background:#ddd;">Model diagnostics</div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Coefficient_of_determination" title="Coefficient of determination">Coefficient of determination</a></li>
<li><a href="/wiki/Confusion_matrix" title="Confusion matrix">Confusion matrix</a></li>
<li><a href="/wiki/Learning_curve_(machine_learning)" title="Learning curve (machine learning)">Learning curve</a></li>
<li><a href="/wiki/Receiver_operating_characteristic" title="Receiver operating characteristic">ROC curve</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="border-top:1px solid #aaa;text-align:center; background:#ddd;">Mathematical foundations</div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Kernel_machines" class="mw-redirect" title="Kernel machines">Kernel machines</a></li>
<li><a href="/wiki/Bias%E2%80%93variance_tradeoff" title="Bias–variance tradeoff">Bias–variance tradeoff</a></li>
<li><a href="/wiki/Computational_learning_theory" title="Computational learning theory">Computational learning theory</a></li>
<li><a href="/wiki/Empirical_risk_minimization" title="Empirical risk minimization">Empirical risk minimization</a></li>
<li><a href="/wiki/Occam_learning" title="Occam learning">Occam learning</a></li>
<li><a href="/wiki/Probably_approximately_correct_learning" title="Probably approximately correct learning">PAC learning</a></li>
<li><a href="/wiki/Statistical_learning_theory" title="Statistical learning theory">Statistical learning</a></li>
<li><a href="/wiki/Vapnik%E2%80%93Chervonenkis_theory" title="Vapnik–Chervonenkis theory">VC theory</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="border-top:1px solid #aaa;text-align:center; background:#ddd;">Machine-learning venues</div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/ECML_PKDD" title="ECML PKDD">ECML PKDD</a></li>
<li><a href="/wiki/Conference_on_Neural_Information_Processing_Systems" title="Conference on Neural Information Processing Systems">NeurIPS</a></li>
<li><a href="/wiki/International_Conference_on_Machine_Learning" title="International Conference on Machine Learning">ICML</a></li>
<li><a href="/wiki/International_Conference_on_Learning_Representations" title="International Conference on Learning Representations">ICLR</a></li>
<li><a href="/wiki/International_Joint_Conference_on_Artificial_Intelligence" title="International Joint Conference on Artificial Intelligence">IJCAI</a></li>
<li><a href="/wiki/Machine_Learning_(journal)" title="Machine Learning (journal)">ML</a></li>
<li><a href="/wiki/Journal_of_Machine_Learning_Research" title="Journal of Machine Learning Research">JMLR</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="border-top:1px solid #aaa;text-align:center; background:#ddd;">Related articles</div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Glossary_of_artificial_intelligence" title="Glossary of artificial intelligence">Glossary of artificial intelligence</a></li>
<li><a href="/wiki/List_of_datasets_for_machine-learning_research" title="List of datasets for machine-learning research">List of datasets for machine-learning research</a>
<ul><li><a href="/wiki/List_of_datasets_in_computer_vision_and_image_processing" title="List of datasets in computer vision and image processing">List of datasets in computer vision and image processing</a></li></ul></li>
<li><a href="/wiki/Outline_of_machine_learning" title="Outline of machine learning">Outline of machine learning</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-navbar"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1129693374"><style data-mw-deduplicate="TemplateStyles:r1063604349">.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:"[ "}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:" ]"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a>span,.mw-parser-output .navbar a>abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}</style><div class="navbar plainlinks hlist navbar-mini"><ul><li class="nv-view"><a href="/wiki/Template:Machine_learning" title="Template:Machine learning"><abbr title="View this template">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Machine_learning" title="Template talk:Machine learning"><abbr title="Discuss this template">t</abbr></a></li><li class="nv-edit"><a href="/wiki/Special:EditPage/Template:Machine_learning" title="Special:EditPage/Template:Machine learning"><abbr title="Edit this template">e</abbr></a></li></ul></div></td></tr></tbody></table>
<p><b>Word2vec</b> is a technique in <a href="/wiki/Natural_language_processing" title="Natural language processing">natural language processing</a> (NLP) for obtaining <a href="/wiki/Vector_space" title="Vector space">vector</a> representations of words. These vectors capture information about the meaning of the word based on the surrounding words. The word2vec algorithm estimates these representations by modeling text in a large <a href="/wiki/Corpus_of_text" class="mw-redirect" title="Corpus of text">corpus</a>. Once trained, such a model can detect <a href="/wiki/Synonym" title="Synonym">synonymous</a> words or suggest additional words for a partial sentence. Word2vec was developed by <a href="/wiki/Tom%C3%A1%C5%A1_Mikolov" title="Tomáš Mikolov">Tomáš Mikolov</a> and colleagues at Google and published in 2013.
</p><p>Word2vec represents a word as a high-dimension <a href="/wiki/Vector_(geometry)" class="mw-redirect" title="Vector (geometry)">vector</a> of numbers which capture relationships between words. In particular, words which appear in similar contexts are mapped to vectors which are nearby as measured by <a href="/wiki/Cosine_similarity" title="Cosine similarity">cosine similarity</a>. This indicates the level of <a href="/wiki/Semantic_similarity" title="Semantic similarity">semantic similarity</a> between the words, so for example the vectors for <i>walk</i> and <i>ran</i> are nearby, as are those for <i>but</i> and <i>however</i> and <i>Berlin</i> and <i>Germany</i>.
</p>
<meta property="mw:PageProp/toc" />
<h2><span class="mw-headline" id="Approach">Approach</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Word2vec&amp;action=edit&amp;section=1" title="Edit section: Approach"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Word2vec is a group of related models that are used to produce <a href="/wiki/Word_embedding" title="Word embedding">word embeddings</a>. These models are shallow, two-layer <a href="/wiki/Neural_network" title="Neural network">neural networks</a> that are trained to reconstruct linguistic contexts of words. Word2vec takes as its input a large <a href="/wiki/Text_corpus" title="Text corpus">corpus of text</a> and produces a <a href="/wiki/Vector_space" title="Vector space">vector space</a>, typically of several hundred <a href="/wiki/Dimensions" class="mw-redirect" title="Dimensions">dimensions</a>, with each unique word in the <a href="/wiki/Corpus_linguistics" title="Corpus linguistics">corpus</a> being assigned a corresponding vector in the space.
</p><p>Word2vec can utilize either of two model architectures to produce these <a href="/wiki/Distributed_representation" class="mw-redirect" title="Distributed representation">distributed representations</a> of words: Continuous <a href="/wiki/Bag-of-words" class="mw-redirect" title="Bag-of-words">Bag-Of-Words</a> (CBOW) or continuously sliding skip-gram. In both architectures, word2vec considers both individual words and a sliding context window as it iterates over the corpus. 
</p><p>The CBOW can be viewed as a ‘fill in the blank’ task, where the word embedding represents the way the word influences the relative probabilities of other words in the context window. Words which are semantically similar should influence these probabilities in similar ways, because semantically similar words should be used in similar contexts. The order of context words does not influence prediction (<a href="/wiki/Bag-of-words" class="mw-redirect" title="Bag-of-words">bag-of-words</a> assumption). 
</p><p>In the continuous skip-gram architecture, the model uses the current word to predict the surrounding window of context words.<sup id="cite_ref-mikolov_1-0" class="reference"><a href="#cite_note-mikolov-1">&#91;1&#93;</a></sup><sup id="cite_ref-mikolov-nips_2-0" class="reference"><a href="#cite_note-mikolov-nips-2">&#91;2&#93;</a></sup> The skip-gram architecture weighs nearby context words more heavily than more distant context words. According to the authors' note,<sup id="cite_ref-:1_3-0" class="reference"><a href="#cite_note-:1-3">&#91;3&#93;</a></sup> CBOW is faster while skip-gram does a better job for infrequent words.
</p><p>After the model has trained, the learned <a href="/wiki/Word_embedding" title="Word embedding">word embeddings</a> are positioned in the vector space such that words that share common contexts in the corpus — that is, words that are <a href="/wiki/Semantic_similarity" title="Semantic similarity">semantically</a> and syntactically similar — are located close to one another in the space.<sup id="cite_ref-mikolov_1-1" class="reference"><a href="#cite_note-mikolov-1">&#91;1&#93;</a></sup> More dissimilar words are located farther from one another in the space.<sup id="cite_ref-mikolov_1-2" class="reference"><a href="#cite_note-mikolov-1">&#91;1&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="Mathematical_details">Mathematical details</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Word2vec&amp;action=edit&amp;section=2" title="Edit section: Mathematical details"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h2>
<p>This section is based on expositions.<sup id="cite_ref-explain_4-0" class="reference"><a href="#cite_note-explain-4">&#91;4&#93;</a></sup><sup id="cite_ref-5" class="reference"><a href="#cite_note-5">&#91;5&#93;</a></sup>
</p><p>A corpus is a sequence of words. Both CBOW and skip-gram are methods to learn one vector per word appearing in the corpus.
</p><p>Let <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle V}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>V</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle V}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/af0f6064540e84211d0ffe4dac72098adfa52845" class="mwe-math-fallback-image-inline mw-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.787ex; height:2.176ex;" alt="{\displaystyle V}"></span> ("vocabulary") be the set of all words appearing in the corpus <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle C}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>C</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle C}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4fc55753007cd3c18576f7933f6f089196732029" class="mwe-math-fallback-image-inline mw-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:1.766ex; height:2.176ex;" alt="{\displaystyle C}"></span>. Our goal is to learn one vector <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle v_{w}\in \mathbb {R} ^{n}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>v</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>w</mi>
          </mrow>
        </msub>
        <mo>&#x2208;<!-- ∈ --></mo>
        <msup>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="double-struck">R</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle v_{w}\in \mathbb {R} ^{n}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/10ffad67d629d0df21f0aac849e8f7ebf8ed370e" class="mwe-math-fallback-image-inline mw-invert" aria-hidden="true" style="vertical-align: -0.671ex; width:8.274ex; height:2.676ex;" alt="{\displaystyle v_{w}\in \mathbb {R} ^{n}}"></span> for each word <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle w\in V}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>w</mi>
        <mo>&#x2208;<!-- ∈ --></mo>
        <mi>V</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle w\in V}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/fb3ba2e494febb4b85886a94ea45400bbfa30176" class="mwe-math-fallback-image-inline mw-invert" aria-hidden="true" style="vertical-align: -0.338ex; width:6.292ex; height:2.176ex;" alt="{\displaystyle w\in V}"></span>.
</p><p>The idea of skip-gram is that the vector of a word should be close to the vector of each of its neighbors. The idea of CBOW is that the vector-sum of a word's neighbors should be close to the vector of the word.
</p><p>In the original publication, "closeness" is measured by <a href="/wiki/Softmax_function" title="Softmax function">softmax</a>, but the framework allows other ways to measure closeness.
</p>
<h3><span id="Continuous_Bag_of_Words_.28CBOW.29"></span><span class="mw-headline" id="Continuous_Bag_of_Words_(CBOW)">Continuous Bag of Words (CBOW)</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Word2vec&amp;action=edit&amp;section=3" title="Edit section: Continuous Bag of Words (CBOW)"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Suppose we want each word in the corpus to be predicted by every other word in a small span of 4 words. We write the neighbor set <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle N=\{-4,-3,-2,-1,+1,+2,+3,+4\}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>N</mi>
        <mo>=</mo>
        <mo fence="false" stretchy="false">{</mo>
        <mo>&#x2212;<!-- − --></mo>
        <mn>4</mn>
        <mo>,</mo>
        <mo>&#x2212;<!-- − --></mo>
        <mn>3</mn>
        <mo>,</mo>
        <mo>&#x2212;<!-- − --></mo>
        <mn>2</mn>
        <mo>,</mo>
        <mo>&#x2212;<!-- − --></mo>
        <mn>1</mn>
        <mo>,</mo>
        <mo>+</mo>
        <mn>1</mn>
        <mo>,</mo>
        <mo>+</mo>
        <mn>2</mn>
        <mo>,</mo>
        <mo>+</mo>
        <mn>3</mn>
        <mo>,</mo>
        <mo>+</mo>
        <mn>4</mn>
        <mo fence="false" stretchy="false">}</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle N=\{-4,-3,-2,-1,+1,+2,+3,+4\}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3cf08e5ff31fae815bc83626efb90748b581d889" class="mwe-math-fallback-image-inline mw-invert" aria-hidden="true" style="vertical-align: -0.838ex; width:38.489ex; height:2.843ex;" alt="{\displaystyle N=\{-4,-3,-2,-1,+1,+2,+3,+4\}}"></span>.
</p><p>Then the training objective is to maximize the following quantity:<div class="mwe-math-element"><div class="mwe-math-mathml-display mwe-math-mathml-a11y" style="display: none;"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \prod _{i\in C}Pr(w_{i}|w_{j}:j\in N+i)}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <munder>
          <mo>&#x220F;<!-- ∏ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>&#x2208;<!-- ∈ --></mo>
            <mi>C</mi>
          </mrow>
        </munder>
        <mi>P</mi>
        <mi>r</mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">|</mo>
        </mrow>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
        <mo>:</mo>
        <mi>j</mi>
        <mo>&#x2208;<!-- ∈ --></mo>
        <mi>N</mi>
        <mo>+</mo>
        <mi>i</mi>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \prod _{i\in C}Pr(w_{i}|w_{j}:j\in N+i)}</annotation>
  </semantics>
</math></div><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d01e9655ff4f4dc63935a2c32a76201d033ec578" class="mwe-math-fallback-image-display mw-invert" aria-hidden="true" style="vertical-align: -3.171ex; width:25.087ex; height:5.676ex;" alt="{\displaystyle \prod _{i\in C}Pr(w_{i}|w_{j}:j\in N+i)}"></div>That is, we want to maximize the total probability for the corpus, as seen by a probability model that uses word neighbors to predict words.
</p><p>Products are numerically unstable, so we convert it by taking the logarithm:<div class="mwe-math-element"><div class="mwe-math-mathml-display mwe-math-mathml-a11y" style="display: none;"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \sum _{i\in C}\ln Pr(w_{i}|w_{j}:j\in N+i)}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <munder>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>&#x2208;<!-- ∈ --></mo>
            <mi>C</mi>
          </mrow>
        </munder>
        <mi>ln</mi>
        <mo>&#x2061;<!-- ⁡ --></mo>
        <mi>P</mi>
        <mi>r</mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">|</mo>
        </mrow>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
        <mo>:</mo>
        <mi>j</mi>
        <mo>&#x2208;<!-- ∈ --></mo>
        <mi>N</mi>
        <mo>+</mo>
        <mi>i</mi>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \sum _{i\in C}\ln Pr(w_{i}|w_{j}:j\in N+i)}</annotation>
  </semantics>
</math></div><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8a0f08208d7816c46cbe9290508347a05c309b8c" class="mwe-math-fallback-image-display mw-invert" aria-hidden="true" style="vertical-align: -3.171ex; width:27.799ex; height:5.676ex;" alt="{\displaystyle \sum _{i\in C}\ln Pr(w_{i}|w_{j}:j\in N+i)}"></div>That is, we maximize the log-probability of the corpus.
</p><p>Our probability model is as follows: Given words <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \{w_{j}:j\in N+i\}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo fence="false" stretchy="false">{</mo>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
        <mo>:</mo>
        <mi>j</mi>
        <mo>&#x2208;<!-- ∈ --></mo>
        <mi>N</mi>
        <mo>+</mo>
        <mi>i</mi>
        <mo fence="false" stretchy="false">}</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \{w_{j}:j\in N+i\}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6d76232dcafbd1024b84f829d8a230bfb7b2a585" class="mwe-math-fallback-image-inline mw-invert" aria-hidden="true" style="vertical-align: -1.005ex; width:16.341ex; height:3.009ex;" alt="{\displaystyle \{w_{j}:j\in N+i\}}"></span>, it takes their vector sum <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle v:=\sum _{j\in N+i}v_{w_{j}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>v</mi>
        <mo>:=</mo>
        <munder>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
            <mo>&#x2208;<!-- ∈ --></mo>
            <mi>N</mi>
            <mo>+</mo>
            <mi>i</mi>
          </mrow>
        </munder>
        <msub>
          <mi>v</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <msub>
              <mi>w</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>j</mi>
              </mrow>
            </msub>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle v:=\sum _{j\in N+i}v_{w_{j}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d0880f6291c995e346e368efdfa48751219d4917" class="mwe-math-fallback-image-inline mw-invert" aria-hidden="true" style="vertical-align: -3.338ex; width:13.59ex; height:5.843ex;" alt="{\displaystyle v:=\sum _{j\in N+i}v_{w_{j}}}"></span>, then take the dot-product-softmax with every other vector sum (this step is similar to the attention mechanism in Transformers), to obtain the probability:<div class="mwe-math-element"><div class="mwe-math-mathml-display mwe-math-mathml-a11y" style="display: none;"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle Pr(w|w_{j}:j\in N+i):={\frac {e^{v_{w}\cdot v}}{\sum _{w\in V}e^{v_{w}\cdot v}}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>P</mi>
        <mi>r</mi>
        <mo stretchy="false">(</mo>
        <mi>w</mi>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">|</mo>
        </mrow>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
        <mo>:</mo>
        <mi>j</mi>
        <mo>&#x2208;<!-- ∈ --></mo>
        <mi>N</mi>
        <mo>+</mo>
        <mi>i</mi>
        <mo stretchy="false">)</mo>
        <mo>:=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <msup>
              <mi>e</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <msub>
                  <mi>v</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>w</mi>
                  </mrow>
                </msub>
                <mo>&#x22C5;<!-- ⋅ --></mo>
                <mi>v</mi>
              </mrow>
            </msup>
            <mrow>
              <munder>
                <mo>&#x2211;<!-- ∑ --></mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>w</mi>
                  <mo>&#x2208;<!-- ∈ --></mo>
                  <mi>V</mi>
                </mrow>
              </munder>
              <msup>
                <mi>e</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <msub>
                    <mi>v</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi>w</mi>
                    </mrow>
                  </msub>
                  <mo>&#x22C5;<!-- ⋅ --></mo>
                  <mi>v</mi>
                </mrow>
              </msup>
            </mrow>
          </mfrac>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle Pr(w|w_{j}:j\in N+i):={\frac {e^{v_{w}\cdot v}}{\sum _{w\in V}e^{v_{w}\cdot v}}}}</annotation>
  </semantics>
</math></div><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/14422ee2f6216bd98888014af089fbcb65865e8d" class="mwe-math-fallback-image-display mw-invert" aria-hidden="true" style="vertical-align: -2.838ex; width:36.609ex; height:6.176ex;" alt="{\displaystyle Pr(w|w_{j}:j\in N+i):={\frac {e^{v_{w}\cdot v}}{\sum _{w\in V}e^{v_{w}\cdot v}}}}"></div>The quantity to be maximized is then after simplifications:<div class="mwe-math-element"><div class="mwe-math-mathml-display mwe-math-mathml-a11y" style="display: none;"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \sum _{i\in C,j\in N+i}\left(v_{w_{i}}\cdot v_{w_{j}}-\ln \sum _{w\in V}e^{v_{w}\cdot v_{w_{j}}}\right)}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <munder>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>&#x2208;<!-- ∈ --></mo>
            <mi>C</mi>
            <mo>,</mo>
            <mi>j</mi>
            <mo>&#x2208;<!-- ∈ --></mo>
            <mi>N</mi>
            <mo>+</mo>
            <mi>i</mi>
          </mrow>
        </munder>
        <mrow>
          <mo>(</mo>
          <mrow>
            <msub>
              <mi>v</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <msub>
                  <mi>w</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>i</mi>
                  </mrow>
                </msub>
              </mrow>
            </msub>
            <mo>&#x22C5;<!-- ⋅ --></mo>
            <msub>
              <mi>v</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <msub>
                  <mi>w</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>j</mi>
                  </mrow>
                </msub>
              </mrow>
            </msub>
            <mo>&#x2212;<!-- − --></mo>
            <mi>ln</mi>
            <mo>&#x2061;<!-- ⁡ --></mo>
            <munder>
              <mo>&#x2211;<!-- ∑ --></mo>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>w</mi>
                <mo>&#x2208;<!-- ∈ --></mo>
                <mi>V</mi>
              </mrow>
            </munder>
            <msup>
              <mi>e</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <msub>
                  <mi>v</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>w</mi>
                  </mrow>
                </msub>
                <mo>&#x22C5;<!-- ⋅ --></mo>
                <msub>
                  <mi>v</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <msub>
                      <mi>w</mi>
                      <mrow class="MJX-TeXAtom-ORD">
                        <mi>j</mi>
                      </mrow>
                    </msub>
                  </mrow>
                </msub>
              </mrow>
            </msup>
          </mrow>
          <mo>)</mo>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \sum _{i\in C,j\in N+i}\left(v_{w_{i}}\cdot v_{w_{j}}-\ln \sum _{w\in V}e^{v_{w}\cdot v_{w_{j}}}\right)}</annotation>
  </semantics>
</math></div><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7f102e56ae989865264a34961621daf74dcca6ae" class="mwe-math-fallback-image-display mw-invert" aria-hidden="true" style="vertical-align: -3.338ex; width:35.99ex; height:7.676ex;" alt="{\displaystyle \sum _{i\in C,j\in N+i}\left(v_{w_{i}}\cdot v_{w_{j}}-\ln \sum _{w\in V}e^{v_{w}\cdot v_{w_{j}}}\right)}"></div>The quantity on the left is fast to compute, but the quantity on the right is slow, as it involves summing over the entire vocabulary set for each word in the corpus. Furthermore, to use gradient ascent to maximize the log-probability requires computing the gradient of the quantity on the right, which is intractable. This prompted the authors to use numerical approximation tricks.
</p>
<h3><span class="mw-headline" id="Skip-gram">Skip-gram</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Word2vec&amp;action=edit&amp;section=4" title="Edit section: Skip-gram"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h3>
<p>For skip-gram, the training objective is<div class="mwe-math-element"><div class="mwe-math-mathml-display mwe-math-mathml-a11y" style="display: none;"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \prod _{i\in C}Pr(w_{j}:j\in N+i|w_{i})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <munder>
          <mo>&#x220F;<!-- ∏ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>&#x2208;<!-- ∈ --></mo>
            <mi>C</mi>
          </mrow>
        </munder>
        <mi>P</mi>
        <mi>r</mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
        <mo>:</mo>
        <mi>j</mi>
        <mo>&#x2208;<!-- ∈ --></mo>
        <mi>N</mi>
        <mo>+</mo>
        <mi>i</mi>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">|</mo>
        </mrow>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \prod _{i\in C}Pr(w_{j}:j\in N+i|w_{i})}</annotation>
  </semantics>
</math></div><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/261774200d8e3e02616047ec592ce9bb7a02f2f7" class="mwe-math-fallback-image-display mw-invert" aria-hidden="true" style="vertical-align: -3.171ex; width:25.087ex; height:5.676ex;" alt="{\displaystyle \prod _{i\in C}Pr(w_{j}:j\in N+i|w_{i})}"></div>That is, we want to maximize the total probability for the corpus, as seen by a probability model that uses words to predict its word neighbors. We predict each word-neighbor independently, thus <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle Pr(w_{j}:j\in N+i|w_{i})=\prod _{j\in N+i}Pr(w_{j}|w_{i})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>P</mi>
        <mi>r</mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
        <mo>:</mo>
        <mi>j</mi>
        <mo>&#x2208;<!-- ∈ --></mo>
        <mi>N</mi>
        <mo>+</mo>
        <mi>i</mi>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">|</mo>
        </mrow>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <munder>
          <mo>&#x220F;<!-- ∏ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
            <mo>&#x2208;<!-- ∈ --></mo>
            <mi>N</mi>
            <mo>+</mo>
            <mi>i</mi>
          </mrow>
        </munder>
        <mi>P</mi>
        <mi>r</mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">|</mo>
        </mrow>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle Pr(w_{j}:j\in N+i|w_{i})=\prod _{j\in N+i}Pr(w_{j}|w_{i})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/410d9a4836057ccbd918ed178f8e9d7f3a70dccb" class="mwe-math-fallback-image-inline mw-invert" aria-hidden="true" style="vertical-align: -3.338ex; width:40.583ex; height:5.843ex;" alt="{\displaystyle Pr(w_{j}:j\in N+i|w_{i})=\prod _{j\in N+i}Pr(w_{j}|w_{i})}"></span>.
</p><p>Products are numerically unstable, so we convert it by taking the logarithm:<div class="mwe-math-element"><div class="mwe-math-mathml-display mwe-math-mathml-a11y" style="display: none;"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \sum _{i\in C,j\in N+i}\ln Pr(w_{j}|w_{i})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <munder>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>&#x2208;<!-- ∈ --></mo>
            <mi>C</mi>
            <mo>,</mo>
            <mi>j</mi>
            <mo>&#x2208;<!-- ∈ --></mo>
            <mi>N</mi>
            <mo>+</mo>
            <mi>i</mi>
          </mrow>
        </munder>
        <mi>ln</mi>
        <mo>&#x2061;<!-- ⁡ --></mo>
        <mi>P</mi>
        <mi>r</mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">|</mo>
        </mrow>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \sum _{i\in C,j\in N+i}\ln Pr(w_{j}|w_{i})}</annotation>
  </semantics>
</math></div><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e75a66653c3ca5eb87704867d882c9c03ab93f99" class="mwe-math-fallback-image-display mw-invert" aria-hidden="true" style="vertical-align: -3.338ex; width:21.45ex; height:5.843ex;" alt="{\displaystyle \sum _{i\in C,j\in N+i}\ln Pr(w_{j}|w_{i})}"></div>The probability model is still the dot-product-softmax model, so the calculation proceeds as before.<div class="mwe-math-element"><div class="mwe-math-mathml-display mwe-math-mathml-a11y" style="display: none;"><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \sum _{i\in C,j\in N+i}\left(v_{w_{i}}\cdot v_{w_{j}}-\ln \sum _{w\in V}e^{v_{w}\cdot v_{w_{\color {red}i}}}\right)}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <munder>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>&#x2208;<!-- ∈ --></mo>
            <mi>C</mi>
            <mo>,</mo>
            <mi>j</mi>
            <mo>&#x2208;<!-- ∈ --></mo>
            <mi>N</mi>
            <mo>+</mo>
            <mi>i</mi>
          </mrow>
        </munder>
        <mrow>
          <mo>(</mo>
          <mrow>
            <msub>
              <mi>v</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <msub>
                  <mi>w</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>i</mi>
                  </mrow>
                </msub>
              </mrow>
            </msub>
            <mo>&#x22C5;<!-- ⋅ --></mo>
            <msub>
              <mi>v</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <msub>
                  <mi>w</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>j</mi>
                  </mrow>
                </msub>
              </mrow>
            </msub>
            <mo>&#x2212;<!-- − --></mo>
            <mi>ln</mi>
            <mo>&#x2061;<!-- ⁡ --></mo>
            <munder>
              <mo>&#x2211;<!-- ∑ --></mo>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>w</mi>
                <mo>&#x2208;<!-- ∈ --></mo>
                <mi>V</mi>
              </mrow>
            </munder>
            <msup>
              <mi>e</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <msub>
                  <mi>v</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>w</mi>
                  </mrow>
                </msub>
                <mo>&#x22C5;<!-- ⋅ --></mo>
                <msub>
                  <mi>v</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <msub>
                      <mi>w</mi>
                      <mrow class="MJX-TeXAtom-ORD">
                        <mstyle mathcolor="red">
                          <mi>i</mi>
                        </mstyle>
                      </mrow>
                    </msub>
                  </mrow>
                </msub>
              </mrow>
            </msup>
          </mrow>
          <mo>)</mo>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \sum _{i\in C,j\in N+i}\left(v_{w_{i}}\cdot v_{w_{j}}-\ln \sum _{w\in V}e^{v_{w}\cdot v_{w_{\color {red}i}}}\right)}</annotation>
  </semantics>
</math></div><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/48240db77495cb7354340e3147020a28afe573dd" class="mwe-math-fallback-image-display mw-invert" aria-hidden="true" style="vertical-align: -3.338ex; width:35.9ex; height:7.676ex;" alt="{\displaystyle \sum _{i\in C,j\in N+i}\left(v_{w_{i}}\cdot v_{w_{j}}-\ln \sum _{w\in V}e^{v_{w}\cdot v_{w_{\color {red}i}}}\right)}"></div>There is only a single difference from the CBOW equation, highlighted in red.
</p>
<h2><span class="mw-headline" id="History">History</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Word2vec&amp;action=edit&amp;section=5" title="Edit section: History"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h2>
<p>In 2010, <a href="/wiki/Tom%C3%A1%C5%A1_Mikolov" title="Tomáš Mikolov">Tomáš Mikolov</a> (then at <a href="/wiki/Brno_University_of_Technology" title="Brno University of Technology">Brno University of Technology</a>) with co-authors applied a simple <a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">recurrent neural network</a> with a single hidden layer to language modelling.<sup id="cite_ref-6" class="reference"><a href="#cite_note-6">&#91;6&#93;</a></sup>
</p><p>Word2vec was created, patented,<sup id="cite_ref-pat_7-0" class="reference"><a href="#cite_note-pat-7">&#91;7&#93;</a></sup> and published in 2013 by a team of researchers led by Mikolov at <a href="/wiki/Google" title="Google">Google</a> over two papers.<sup id="cite_ref-mikolov_1-3" class="reference"><a href="#cite_note-mikolov-1">&#91;1&#93;</a></sup><sup id="cite_ref-mikolov-nips_2-1" class="reference"><a href="#cite_note-mikolov-nips-2">&#91;2&#93;</a></sup> Other researchers helped analyse and explain the algorithm.<sup id="cite_ref-explain_4-1" class="reference"><a href="#cite_note-explain-4">&#91;4&#93;</a></sup> Embedding vectors created using the Word2vec algorithm have some advantages compared to earlier algorithms<sup id="cite_ref-mikolov_1-4" class="reference"><a href="#cite_note-mikolov-1">&#91;1&#93;</a></sup><sup class="noprint Inline-Template" style="white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Please_clarify" title="Wikipedia:Please clarify"><span title="The text near this tag needs further explanation. (February 2022)">further explanation needed</span></a></i>&#93;</sup> such as <a href="/wiki/Latent_semantic_analysis" title="Latent semantic analysis">latent semantic analysis</a>.
</p><p>By 2022, the straight Word2vec approach was described as "dated." <a href="/wiki/Transformer_(machine_learning_model)" class="mw-redirect" title="Transformer (machine learning model)">Transformer models</a>, which add multiple neural-network attention layers on top of a word embedding model similar to Word2vec, have come to be regarded as the state of the art in NLP.<sup id="cite_ref-8" class="reference"><a href="#cite_note-8">&#91;8&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="Parameterization">Parameterization</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Word2vec&amp;action=edit&amp;section=6" title="Edit section: Parameterization"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Results of word2vec training can be sensitive to <a href="/wiki/Parameter" title="Parameter">parametrization</a>. The following are some important parameters in word2vec training.
</p>
<h3><span class="mw-headline" id="Training_algorithm">Training algorithm</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Word2vec&amp;action=edit&amp;section=7" title="Edit section: Training algorithm"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h3>
<p>A Word2vec model can be trained with hierarchical <a href="/wiki/Softmax_function" title="Softmax function">softmax</a> and/or negative sampling. To approximate the conditional log-likelihood a model seeks to maximize, the hierarchical softmax method uses a <a href="/wiki/Huffman_coding" title="Huffman coding">Huffman tree</a> to reduce calculation. The negative sampling method, on the other hand, approaches the maximization problem by minimizing the <a href="/wiki/Log-likelihood" class="mw-redirect" title="Log-likelihood">log-likelihood</a> of sampled negative instances. According to the authors, hierarchical softmax works better for infrequent words while negative sampling works better for frequent words and better with low dimensional vectors.<sup id="cite_ref-:1_3-1" class="reference"><a href="#cite_note-:1-3">&#91;3&#93;</a></sup> As training epochs increase, hierarchical softmax stops being useful.<sup id="cite_ref-9" class="reference"><a href="#cite_note-9">&#91;9&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Sub-sampling">Sub-sampling</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Word2vec&amp;action=edit&amp;section=8" title="Edit section: Sub-sampling"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h3>
<p>High-frequency and low-frequency words often provide little information. Words with a frequency above a certain threshold, or below a certain threshold, may be subsampled or removed to speed up training.<sup id="cite_ref-10" class="reference"><a href="#cite_note-10">&#91;10&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Dimensionality">Dimensionality</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Word2vec&amp;action=edit&amp;section=9" title="Edit section: Dimensionality"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Quality of word embedding increases with higher dimensionality. But after reaching some point, marginal gain diminishes.<sup id="cite_ref-mikolov_1-5" class="reference"><a href="#cite_note-mikolov-1">&#91;1&#93;</a></sup> Typically, the dimensionality of the vectors is set to be between 100 and 1,000.
</p>
<h3><span class="mw-headline" id="Context_window">Context window</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Word2vec&amp;action=edit&amp;section=10" title="Edit section: Context window"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The size of the context window determines how many words before and after a given word are included as context words of the given word. According to the authors' note, the recommended value is 10 for skip-gram and 5 for CBOW.<sup id="cite_ref-:1_3-2" class="reference"><a href="#cite_note-:1-3">&#91;3&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="Extensions">Extensions</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Word2vec&amp;action=edit&amp;section=11" title="Edit section: Extensions"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h2>
<p>There are a variety of extensions to word2vec.
</p>
<h3><span class="mw-headline" id="doc2vec">doc2vec</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Word2vec&amp;action=edit&amp;section=12" title="Edit section: doc2vec"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h3>
<p>doc2vec, generates distributed representations of <i>variable-length</i> pieces of texts, such as sentences, paragraphs, or entire documents.<sup id="cite_ref-:2_11-0" class="reference"><a href="#cite_note-:2-11">&#91;11&#93;</a></sup><sup id="cite_ref-12" class="reference"><a href="#cite_note-12">&#91;12&#93;</a></sup> doc2vec has been implemented in the <a href="/wiki/C_(programming_language)" title="C (programming language)">C</a>, <a href="/wiki/Python_(programming_language)" title="Python (programming language)">Python</a> and <a href="/wiki/Java_(programming_language)" title="Java (programming language)">Java</a>/<a href="/wiki/Scala_(programming_language)" title="Scala (programming language)">Scala</a> tools (see below), with the Java and Python versions also supporting inference of document embeddings on new, unseen documents.
</p><p>doc2vec estimates the distributed representations of documents much like how word2vec estimates representations of words: doc2vec utilizes either of two model architectures, both of which are allegories to the architectures used in word2vec. The first, Distributed Memory Model of Paragraph Vectors (PV-DM), is identical to CBOW other than it also provides a unique document identifier as a piece of additional context. The second architecture, Distributed Bag of Words version of Paragraph Vector (PV-DBOW), is identical to the skip-gram model except that it attempts to predict the window of surrounding context words from the paragraph identifier instead of the current word.<sup id="cite_ref-:2_11-1" class="reference"><a href="#cite_note-:2-11">&#91;11&#93;</a></sup>
</p><p>doc2vec also has the ability to capture the semantic ‘meanings’ for additional pieces of&#160; ‘context’ around words; doc2vec can estimate the semantic embeddings for speakers or speaker attributes, groups, and periods of time. For example, doc2vec has been used to estimate the political positions of political parties in various Congresses and Parliaments in the U.S. and U.K.,<sup id="cite_ref-13" class="reference"><a href="#cite_note-13">&#91;13&#93;</a></sup> respectively, and various governmental institutions.<sup id="cite_ref-14" class="reference"><a href="#cite_note-14">&#91;14&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="top2vec">top2vec</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Word2vec&amp;action=edit&amp;section=13" title="Edit section: top2vec"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Another extension of word2vec is top2vec, which leverages both document and word embeddings to estimate distributed representations of topics.<sup id="cite_ref-:3_15-0" class="reference"><a href="#cite_note-:3-15">&#91;15&#93;</a></sup><sup id="cite_ref-16" class="reference"><a href="#cite_note-16">&#91;16&#93;</a></sup> top2vec takes document embeddings learned from a doc2vec model and <a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">reduces</a> them into a lower dimension (typically using <a href="/wiki/Uniform_Manifold_Approximation_and_Projection" class="mw-redirect" title="Uniform Manifold Approximation and Projection">UMAP</a>). The space of documents is then scanned using HDBSCAN,<sup id="cite_ref-17" class="reference"><a href="#cite_note-17">&#91;17&#93;</a></sup> and clusters of similar documents are found. Next, the centroid of documents identified in a cluster is considered to be that cluster's topic vector. Finally, top2vec searches the semantic space for word embeddings located near to the topic vector to ascertain the 'meaning' of the topic.<sup id="cite_ref-:3_15-1" class="reference"><a href="#cite_note-:3-15">&#91;15&#93;</a></sup> The word with embeddings most similar to the topic vector might be assigned as the topic's title, whereas far away word embeddings may be considered unrelated.
</p><p>As opposed to other topic models such as <a href="/wiki/Latent_Dirichlet_allocation" title="Latent Dirichlet allocation">LDA</a>, top2vec provides canonical ‘distance’ metrics between two topics, or between a topic and another embeddings (word, document, or otherwise). Together with results from HDBSCAN, users can generate topic hierarchies, or groups of related topics and subtopics.
</p><p>Furthermore, a user can use the results of top2vec to infer the topics of out-of-sample documents. After inferring the embedding for a new document, must only search the space of topics for the closest topic vector.
</p>
<h3><span class="mw-headline" id="BioVectors">BioVectors</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Word2vec&amp;action=edit&amp;section=14" title="Edit section: BioVectors"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h3>
<p>An extension of word vectors for n-grams in <a href="/wiki/Biological" class="mw-redirect" title="Biological">biological</a> sequences (e.g. <a href="/wiki/DNA" title="DNA">DNA</a>, <a href="/wiki/RNA" title="RNA">RNA</a>, and <a href="/wiki/Protein" title="Protein">proteins</a>) for <a href="/wiki/Bioinformatic" class="mw-redirect" title="Bioinformatic">bioinformatics</a> applications has been proposed by Asgari and Mofrad.<sup id="cite_ref-:0_18-0" class="reference"><a href="#cite_note-:0-18">&#91;18&#93;</a></sup> Named bio-vectors (<a href="/w/index.php?title=BioVec&amp;action=edit&amp;redlink=1" class="new" title="BioVec (page does not exist)">BioVec</a>) to refer to biological sequences in general with protein-vectors (ProtVec) for proteins (amino-acid sequences) and gene-vectors (GeneVec) for gene sequences, this representation can be widely used in applications of machine learning in proteomics and genomics. The results suggest that BioVectors can characterize biological sequences in terms of biochemical and biophysical interpretations of the underlying patterns.<sup id="cite_ref-:0_18-1" class="reference"><a href="#cite_note-:0-18">&#91;18&#93;</a></sup> A similar variant, dna2vec, has shown that there is correlation between <a href="/wiki/Needleman%E2%80%93Wunsch_algorithm" title="Needleman–Wunsch algorithm">Needleman–Wunsch</a> similarity score and <a href="/wiki/Cosine_similarity" title="Cosine similarity">cosine similarity</a> of dna2vec word vectors.<sup id="cite_ref-19" class="reference"><a href="#cite_note-19">&#91;19&#93;</a></sup>
</p>
<h3><span id="Radiology_and_intelligent_word_embeddings_.28IWE.29"></span><span class="mw-headline" id="Radiology_and_intelligent_word_embeddings_(IWE)">Radiology and intelligent word embeddings (IWE)</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Word2vec&amp;action=edit&amp;section=15" title="Edit section: Radiology and intelligent word embeddings (IWE)"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h3>
<p>An extension of word vectors for creating a dense vector representation of unstructured radiology reports has been proposed by Banerjee et al.<sup id="cite_ref-20" class="reference"><a href="#cite_note-20">&#91;20&#93;</a></sup> One of the biggest challenges with Word2vec is how to handle unknown or <a href="/w/index.php?title=Out-of-vocabulary&amp;action=edit&amp;redlink=1" class="new" title="Out-of-vocabulary (page does not exist)">out-of-vocabulary</a> (OOV) words and morphologically similar words. If the Word2vec model has not encountered a particular word before, it will be forced to use a random vector, which is generally far from its ideal representation. This can particularly be an issue in domains like medicine where synonyms and related words can be used depending on the preferred style of radiologist, and words may have been used infrequently in a large corpus.
</p><p>IWE combines Word2vec with a semantic dictionary mapping technique to tackle the major challenges of <a href="/wiki/Information_extraction" title="Information extraction">information extraction</a> from clinical texts, which include ambiguity of free text narrative style, lexical variations, use of ungrammatical and telegraphic phases, arbitrary ordering of words, and frequent appearance of abbreviations and acronyms.  Of particular interest, the IWE model (trained on the one institutional dataset) successfully translated to a different institutional dataset which demonstrates good generalizability of the approach across institutions.
</p>
<h2><span class="mw-headline" id="Analysis">Analysis</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Word2vec&amp;action=edit&amp;section=16" title="Edit section: Analysis"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h2>
<p>The reasons for successful <a href="/wiki/Word_embedding" title="Word embedding">word embedding</a> learning in the word2vec framework are poorly understood. Goldberg and Levy point out that the word2vec objective function causes words that occur in similar contexts to have similar embeddings (as measured by <a href="/wiki/Cosine_similarity" title="Cosine similarity">cosine similarity</a>) and note that this is in line with J. R. Firth's <a href="/wiki/Distributional_semantics" title="Distributional semantics">distributional hypothesis</a>. However, they note that this explanation is "very hand-wavy" and argue that a more formal explanation would be preferable.<sup id="cite_ref-explain_4-2" class="reference"><a href="#cite_note-explain-4">&#91;4&#93;</a></sup>
</p><p>Levy et al. (2015)<sup id="cite_ref-21" class="reference"><a href="#cite_note-21">&#91;21&#93;</a></sup> show that much of the superior performance of word2vec or similar embeddings in downstream tasks is not a result of the models per se, but of the choice of specific hyperparameters. Transferring these hyperparameters to more 'traditional' approaches yields similar performances in downstream tasks. Arora et al. (2016)<sup id="cite_ref-22" class="reference"><a href="#cite_note-22">&#91;22&#93;</a></sup> explain word2vec and related algorithms as performing inference for a simple <a href="/wiki/Generative_model" title="Generative model">generative model</a> for text, which involves a random walk generation process based upon loglinear topic model. They use this to explain some properties of word embeddings, including their use to solve analogies.
</p>
<h2><span class="mw-headline" id="Preservation_of_semantic_and_syntactic_relationships">Preservation of semantic and syntactic relationships</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Word2vec&amp;action=edit&amp;section=17" title="Edit section: Preservation of semantic and syntactic relationships"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h2>
<figure class="mw-default-size" typeof="mw:File/Thumb"><a href="/wiki/File:Word_vector_illustration.jpg" class="mw-file-description"><img alt="Visual illustration of word embeddings" src="//upload.wikimedia.org/wikipedia/commons/thumb/3/3f/Word_vector_illustration.jpg/220px-Word_vector_illustration.jpg" decoding="async" width="220" height="220" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/3/3f/Word_vector_illustration.jpg/330px-Word_vector_illustration.jpg 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/3/3f/Word_vector_illustration.jpg/440px-Word_vector_illustration.jpg 2x" data-file-width="1080" data-file-height="1080" /></a><figcaption>Visual illustration of word embeddings</figcaption></figure>
<p>The word embedding approach is able to capture multiple different degrees of similarity between words. Mikolov et al. (2013)<sup id="cite_ref-23" class="reference"><a href="#cite_note-23">&#91;23&#93;</a></sup> found that semantic and syntactic patterns can be reproduced using vector arithmetic. Patterns such as "Man is to Woman as Brother is to Sister" can be generated through algebraic operations on the vector representations of these words such that the vector representation of "Brother" - "Man" + "Woman" produces a result which is closest to the vector representation of "Sister" in the model. Such relationships can be generated for a range of semantic relations (such as Country–Capital) as well as syntactic relations (e.g. present tense–past tense).
</p><p>This facet of word2vec has been exploited in a variety of other contexts. For example, word2vec has been used to map a vector space of words in one language to a vector space constructed from another language. Relationships between translated words in both spaces can be used to assist with <a href="/wiki/Machine_translation" title="Machine translation">machine translation</a> of new words.<sup id="cite_ref-24" class="reference"><a href="#cite_note-24">&#91;24&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="Assessing_the_quality_of_a_model">Assessing the quality of a model</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Word2vec&amp;action=edit&amp;section=18" title="Edit section: Assessing the quality of a model"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Mikolov et al. (2013)<sup id="cite_ref-mikolov_1-6" class="reference"><a href="#cite_note-mikolov-1">&#91;1&#93;</a></sup> develop an approach to assessing the quality of a word2vec model which draws on the semantic and syntactic patterns discussed above. They developed a set of 8,869 semantic relations and 10,675 syntactic relations which they use as a benchmark to test the accuracy of a model. When assessing the quality of a vector model, a user may draw on this accuracy test which is implemented in word2vec,<sup id="cite_ref-25" class="reference"><a href="#cite_note-25">&#91;25&#93;</a></sup> or develop their own test set which is meaningful to the corpora which make up the model. This approach offers a more challenging test than simply arguing that the words most similar to a given test word are intuitively plausible.<sup id="cite_ref-mikolov_1-7" class="reference"><a href="#cite_note-mikolov-1">&#91;1&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Parameters_and_model_quality">Parameters and model quality</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Word2vec&amp;action=edit&amp;section=19" title="Edit section: Parameters and model quality"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The use of different model parameters and different corpus sizes can greatly affect the quality of a word2vec model. Accuracy can be improved in a number of ways, including the choice of model architecture (CBOW or Skip-Gram), increasing the training data set, increasing the number of vector dimensions, and increasing the window size of words considered by the algorithm. Each of these improvements comes with the cost of increased computational complexity and therefore increased model generation time.<sup id="cite_ref-mikolov_1-8" class="reference"><a href="#cite_note-mikolov-1">&#91;1&#93;</a></sup>
</p><p>In models using large corpora and a high number of dimensions, the skip-gram model yields the highest overall accuracy, and consistently produces the highest accuracy on semantic relationships, as well as yielding the highest syntactic accuracy in most cases. However, the CBOW is less computationally expensive and yields similar accuracy results.<sup id="cite_ref-mikolov_1-9" class="reference"><a href="#cite_note-mikolov-1">&#91;1&#93;</a></sup>
</p><p>Overall, accuracy increases with the number of words used and the number of dimensions. Mikolov et al.<sup id="cite_ref-mikolov_1-10" class="reference"><a href="#cite_note-mikolov-1">&#91;1&#93;</a></sup> report that doubling the amount of training data results in an increase in computational complexity equivalent to doubling the number of vector dimensions.
</p><p>Altszyler and coauthors (2017) studied Word2vec performance in two semantic tests for different corpus size.<sup id="cite_ref-Altszyler_26-0" class="reference"><a href="#cite_note-Altszyler-26">&#91;26&#93;</a></sup> They found that Word2vec has a steep <a href="/wiki/Learning_curve_(machine_learning)" title="Learning curve (machine learning)">learning curve</a>, outperforming another word-embedding technique, <a href="/wiki/Latent_semantic_analysis" title="Latent semantic analysis">latent semantic analysis</a> (LSA), when it is trained with medium to large corpus size (more than 10 million words). However, with a small training corpus, LSA showed better performance. Additionally they show that the best parameter setting depends on the task and the training corpus. Nevertheless, for skip-gram models trained in medium size corpora, with 50 dimensions, a window size of 15 and 10 negative samples seems to be a good parameter setting.
</p>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Word2vec&amp;action=edit&amp;section=20" title="Edit section: See also"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h2>
<style data-mw-deduplicate="TemplateStyles:r1184024115">.mw-parser-output .div-col{margin-top:0.3em;column-width:30em}.mw-parser-output .div-col-small{font-size:90%}.mw-parser-output .div-col-rules{column-rule:1px solid #aaa}.mw-parser-output .div-col dl,.mw-parser-output .div-col ol,.mw-parser-output .div-col ul{margin-top:0}.mw-parser-output .div-col li,.mw-parser-output .div-col dd{page-break-inside:avoid;break-inside:avoid-column}</style><div class="div-col" style="column-width: 20em;">
<ul><li><a href="/wiki/Semantle" class="mw-redirect" title="Semantle">Semantle</a></li>
<li><a href="/wiki/Autoencoder" title="Autoencoder">Autoencoder</a></li>
<li><a href="/wiki/Document-term_matrix" title="Document-term matrix">Document-term matrix</a></li>
<li><a href="/wiki/Feature_extraction" class="mw-redirect" title="Feature extraction">Feature extraction</a></li>
<li><a href="/wiki/Feature_learning" title="Feature learning">Feature learning</a></li>
<li><a href="/wiki/Language_model#Neural_network" title="Language model">Neural network language models</a></li>
<li><a href="/wiki/Vector_space_model" title="Vector space model">Vector space model</a></li>
<li><a href="/wiki/Thought_vector" title="Thought vector">Thought vector</a></li>
<li><a href="/wiki/FastText" title="FastText">fastText</a></li>
<li><a href="/wiki/GloVe_(machine_learning)" class="mw-redirect" title="GloVe (machine learning)">GloVe</a></li>
<li><a href="/wiki/Normalized_compression_distance" title="Normalized compression distance">Normalized compression distance</a></li></ul>
</div>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Word2vec&amp;action=edit&amp;section=21" title="Edit section: References"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h2>
<style data-mw-deduplicate="TemplateStyles:r1217336898">.mw-parser-output .reflist{font-size:90%;margin-bottom:0.5em;list-style-type:decimal}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman}</style><div class="reflist">
<div class="mw-references-wrap mw-references-columns"><ol class="references">
<li id="cite_note-mikolov-1"><span class="mw-cite-backlink">^ <a href="#cite_ref-mikolov_1-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-mikolov_1-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-mikolov_1-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-mikolov_1-3"><sup><i><b>d</b></i></sup></a> <a href="#cite_ref-mikolov_1-4"><sup><i><b>e</b></i></sup></a> <a href="#cite_ref-mikolov_1-5"><sup><i><b>f</b></i></sup></a> <a href="#cite_ref-mikolov_1-6"><sup><i><b>g</b></i></sup></a> <a href="#cite_ref-mikolov_1-7"><sup><i><b>h</b></i></sup></a> <a href="#cite_ref-mikolov_1-8"><sup><i><b>i</b></i></sup></a> <a href="#cite_ref-mikolov_1-9"><sup><i><b>j</b></i></sup></a> <a href="#cite_ref-mikolov_1-10"><sup><i><b>k</b></i></sup></a></span> <span class="reference-text"><style data-mw-deduplicate="TemplateStyles:r1215172403">.mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free.id-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-free a{background-size:contain}.mw-parser-output .id-lock-limited.id-lock-limited a,.mw-parser-output .id-lock-registration.id-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-limited a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-registration a{background-size:contain}.mw-parser-output .id-lock-subscription.id-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-subscription a{background-size:contain}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .cs1-ws-icon a{background-size:contain}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#2C882D;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}html.skin-theme-clientpref-night .mw-parser-output .cs1-maint{color:#18911F}html.skin-theme-clientpref-night .mw-parser-output .cs1-visible-error,html.skin-theme-clientpref-night .mw-parser-output .cs1-hidden-error{color:#f8a397}@media(prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .cs1-visible-error,html.skin-theme-clientpref-os .mw-parser-output .cs1-hidden-error{color:#f8a397}html.skin-theme-clientpref-os .mw-parser-output .cs1-maint{color:#18911F}}</style><cite id="CITEREFMikolov2013" class="citation arxiv cs1">Mikolov, Tomas; et&#160;al. (2013). "Efficient Estimation of Word Representations in Vector Space". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1301.3781">1301.3781</a></span> [<a rel="nofollow" class="external text" href="https://arxiv.org/archive/cs.CL">cs.CL</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Efficient+Estimation+of+Word+Representations+in+Vector+Space&amp;rft.date=2013&amp;rft_id=info%3Aarxiv%2F1301.3781&amp;rft.aulast=Mikolov&amp;rft.aufirst=Tomas&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWord2vec" class="Z3988"></span></span>
</li>
<li id="cite_note-mikolov-nips-2"><span class="mw-cite-backlink">^ <a href="#cite_ref-mikolov-nips_2-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-mikolov-nips_2-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFMikolovSutskeverChenCorrado2013" class="citation conference cs1">Mikolov, Tomas; Sutskever, Ilya; Chen, Kai; Corrado, Greg S.; Dean, Jeff (2013). <i>Distributed representations of words and phrases and their compositionality</i>. <a href="/wiki/Advances_in_Neural_Information_Processing_Systems" class="mw-redirect" title="Advances in Neural Information Processing Systems">Advances in Neural Information Processing Systems</a>. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1310.4546">1310.4546</a></span>. <a href="/wiki/Bibcode_(identifier)" class="mw-redirect" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/2013arXiv1310.4546M">2013arXiv1310.4546M</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=Distributed+representations+of+words+and+phrases+and+their+compositionality&amp;rft.date=2013&amp;rft_id=info%3Aarxiv%2F1310.4546&amp;rft_id=info%3Abibcode%2F2013arXiv1310.4546M&amp;rft.aulast=Mikolov&amp;rft.aufirst=Tomas&amp;rft.au=Sutskever%2C+Ilya&amp;rft.au=Chen%2C+Kai&amp;rft.au=Corrado%2C+Greg+S.&amp;rft.au=Dean%2C+Jeff&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWord2vec" class="Z3988"></span></span>
</li>
<li id="cite_note-:1-3"><span class="mw-cite-backlink">^ <a href="#cite_ref-:1_3-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:1_3-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-:1_3-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://code.google.com/archive/p/word2vec/">"Google Code Archive - Long-term storage for Google Code Project Hosting"</a>. <i>code.google.com</i><span class="reference-accessdate">. Retrieved <span class="nowrap">13 June</span> 2016</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=code.google.com&amp;rft.atitle=Google+Code+Archive+-+Long-term+storage+for+Google+Code+Project+Hosting.&amp;rft_id=https%3A%2F%2Fcode.google.com%2Farchive%2Fp%2Fword2vec%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWord2vec" class="Z3988"></span></span>
</li>
<li id="cite_note-explain-4"><span class="mw-cite-backlink">^ <a href="#cite_ref-explain_4-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-explain_4-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-explain_4-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFGoldbergLevy2014" class="citation arxiv cs1">Goldberg, Yoav; Levy, Omer (2014). "word2vec Explained: Deriving Mikolov et al.'s Negative-Sampling Word-Embedding Method". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1402.3722">1402.3722</a></span> [<a rel="nofollow" class="external text" href="https://arxiv.org/archive/cs.CL">cs.CL</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=word2vec+Explained%3A+Deriving+Mikolov+et+al.%27s+Negative-Sampling+Word-Embedding+Method&amp;rft.date=2014&amp;rft_id=info%3Aarxiv%2F1402.3722&amp;rft.aulast=Goldberg&amp;rft.aufirst=Yoav&amp;rft.au=Levy%2C+Omer&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWord2vec" class="Z3988"></span></span>
</li>
<li id="cite_note-5"><span class="mw-cite-backlink"><b><a href="#cite_ref-5">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFRong2016" class="citation cs2">Rong, Xin (5 June 2016), <i>word2vec Parameter Learning Explained</i>, <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1411.2738">1411.2738</a></span></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=word2vec+Parameter+Learning+Explained&amp;rft.date=2016-06-05&amp;rft_id=info%3Aarxiv%2F1411.2738&amp;rft.aulast=Rong&amp;rft.aufirst=Xin&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWord2vec" class="Z3988"></span></span>
</li>
<li id="cite_note-6"><span class="mw-cite-backlink"><b><a href="#cite_ref-6">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFMikolovKarafiátBurgetČernocký2010" class="citation conference cs1">Mikolov, Tomáš; Karafiát, Martin; Burget, Lukáš; Černocký, Jan; Khudanpur, Sanjeev (26 September 2010). "Recurrent neural network based language model". <i>Interspeech 2010</i>. ISCA: ISCA. pp.&#160;1045–1048. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.21437%2Finterspeech.2010-343">10.21437/interspeech.2010-343</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=Recurrent+neural+network+based+language+model&amp;rft.btitle=Interspeech+2010&amp;rft.place=ISCA&amp;rft.pages=1045-1048&amp;rft.pub=ISCA&amp;rft.date=2010-09-26&amp;rft_id=info%3Adoi%2F10.21437%2Finterspeech.2010-343&amp;rft.aulast=Mikolov&amp;rft.aufirst=Tom%C3%A1%C5%A1&amp;rft.au=Karafi%C3%A1t%2C+Martin&amp;rft.au=Burget%2C+Luk%C3%A1%C5%A1&amp;rft.au=%C4%8Cernock%C3%BD%2C+Jan&amp;rft.au=Khudanpur%2C+Sanjeev&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWord2vec" class="Z3988"></span></span>
</li>
<li id="cite_note-pat-7"><span class="mw-cite-backlink"><b><a href="#cite_ref-pat_7-0">^</a></b></span> <span class="reference-text"><style data-mw-deduplicate="TemplateStyles:r1041539562">.mw-parser-output .citation{word-wrap:break-word}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}</style><span class="citation patent" id="CITEREFMikolovChenCorradoDean2015"><a rel="nofollow" class="external text" href="https://worldwide.espacenet.com/textdoc?DB=EPODOC&amp;IDX=US9037464">US 9037464</a>,&#32;Mikolov, Tomas; Chen, Kai&#32;&amp; Corrado, Gregory S.&#32;et al.,&#32;"Computing numeric representations of words in a high-dimensional space",&#32;published 2015-05-19,&#32; assigned to <a href="/wiki/Google_Inc." class="mw-redirect" title="Google Inc.">Google Inc.</a></span><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Apatent&amp;rft.number=9037464&amp;rft.cc=US&amp;rft.title=Computing+numeric+representations+of+words+in+a+high-dimensional+space&amp;rft.inventor=Mikolov&amp;rft.assignee=%5B%5BGoogle+Inc.%5D%5D&amp;rft.pubdate=2015-05-19"><span style="display: none;">&#160;</span></span></span>
</li>
<li id="cite_note-8"><span class="mw-cite-backlink"><b><a href="#cite_ref-8">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFVon_der_MoselTrautschHerbold2022" class="citation journal cs1">Von der Mosel, Julian; Trautsch, Alexander; Herbold, Steffen (2022). <a rel="nofollow" class="external text" href="https://ieeexplore.ieee.org/document/9785808">"On the validity of pre-trained transformers for natural language processing in the software engineering domain"</a>. <i>IEEE Transactions on Software Engineering</i>. <b>49</b> (4): 1487–1507. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2109.04738">2109.04738</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1109%2FTSE.2022.3178469">10.1109/TSE.2022.3178469</a>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://www.worldcat.org/issn/1939-3520">1939-3520</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:237485425">237485425</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Transactions+on+Software+Engineering&amp;rft.atitle=On+the+validity+of+pre-trained+transformers+for+natural+language+processing+in+the+software+engineering+domain&amp;rft.volume=49&amp;rft.issue=4&amp;rft.pages=1487-1507&amp;rft.date=2022&amp;rft_id=info%3Aarxiv%2F2109.04738&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A237485425%23id-name%3DS2CID&amp;rft.issn=1939-3520&amp;rft_id=info%3Adoi%2F10.1109%2FTSE.2022.3178469&amp;rft.aulast=Von+der+Mosel&amp;rft.aufirst=Julian&amp;rft.au=Trautsch%2C+Alexander&amp;rft.au=Herbold%2C+Steffen&amp;rft_id=https%3A%2F%2Fieeexplore.ieee.org%2Fdocument%2F9785808&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWord2vec" class="Z3988"></span></span>
</li>
<li id="cite_note-9"><span class="mw-cite-backlink"><b><a href="#cite_ref-9">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://groups.google.com/forum/#!msg/word2vec-toolkit/WUWad9fL0jU/LdbWy1jQjUIJ">"Parameter (hs &amp; negative)"</a>. <i>Google Groups</i><span class="reference-accessdate">. Retrieved <span class="nowrap">13 June</span> 2016</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Google+Groups&amp;rft.atitle=Parameter+%28hs+%26+negative%29&amp;rft_id=https%3A%2F%2Fgroups.google.com%2Fforum%2F%23%21msg%2Fword2vec-toolkit%2FWUWad9fL0jU%2FLdbWy1jQjUIJ&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWord2vec" class="Z3988"></span></span>
</li>
<li id="cite_note-10"><span class="mw-cite-backlink"><b><a href="#cite_ref-10">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite class="citation web cs1"><a rel="nofollow" class="external text" href="http://jmlr.csail.mit.edu/papers/volume9/vandermaaten08a/vandermaaten08a.pdf">"Visualizing Data using t-SNE"</a> <span class="cs1-format">(PDF)</span>. <i>Journal of Machine Learning Research, 2008. Vol. 9, pg. 2595</i><span class="reference-accessdate">. Retrieved <span class="nowrap">18 March</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Journal+of+Machine+Learning+Research%2C+2008.+Vol.+9%2C+pg.+2595&amp;rft.atitle=Visualizing+Data+using+t-SNE&amp;rft_id=http%3A%2F%2Fjmlr.csail.mit.edu%2Fpapers%2Fvolume9%2Fvandermaaten08a%2Fvandermaaten08a.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWord2vec" class="Z3988"></span></span>
</li>
<li id="cite_note-:2-11"><span class="mw-cite-backlink">^ <a href="#cite_ref-:2_11-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:2_11-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFLeMikolov2014" class="citation journal cs1">Le, Quoc; Mikolov, Tomas (May 2014). "Distributed Representations of Sentences and Documents". <i>Proceedings of the 31st International Conference on Machine Learning</i>. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1405.4053">1405.4053</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proceedings+of+the+31st+International+Conference+on+Machine+Learning&amp;rft.atitle=Distributed+Representations+of+Sentences+and+Documents&amp;rft.date=2014-05&amp;rft_id=info%3Aarxiv%2F1405.4053&amp;rft.aulast=Le&amp;rft.aufirst=Quoc&amp;rft.au=Mikolov%2C+Tomas&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWord2vec" class="Z3988"></span></span>
</li>
<li id="cite_note-12"><span class="mw-cite-backlink"><b><a href="#cite_ref-12">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFRehurek" class="citation web cs1">Rehurek, Radim. <a rel="nofollow" class="external text" href="https://radimrehurek.com/gensim/models/doc2vec.html">"Gensim"</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Gensim&amp;rft.aulast=Rehurek&amp;rft.aufirst=Radim&amp;rft_id=https%3A%2F%2Fradimrehurek.com%2Fgensim%2Fmodels%2Fdoc2vec.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWord2vec" class="Z3988"></span></span>
</li>
<li id="cite_note-13"><span class="mw-cite-backlink"><b><a href="#cite_ref-13">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFRheaultCochrane2019" class="citation journal cs1">Rheault, Ludovic; Cochrane, Christopher (3 July 2019). <a rel="nofollow" class="external text" href="https://www.cambridge.org/core/journals/political-analysis/article/word-embeddings-for-the-analysis-of-ideological-placement-in-parliamentary-corpora/017F0CEA9B3DB6E1B94AC36A509A8A7B">"Word Embeddings for the Analysis of Ideological Placement in Parliamentary Corpora"</a>. <i>Political Analysis</i>. <b>28</b> (1).</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Political+Analysis&amp;rft.atitle=Word+Embeddings+for+the+Analysis+of+Ideological+Placement+in+Parliamentary+Corpora&amp;rft.volume=28&amp;rft.issue=1&amp;rft.date=2019-07-03&amp;rft.aulast=Rheault&amp;rft.aufirst=Ludovic&amp;rft.au=Cochrane%2C+Christopher&amp;rft_id=https%3A%2F%2Fwww.cambridge.org%2Fcore%2Fjournals%2Fpolitical-analysis%2Farticle%2Fword-embeddings-for-the-analysis-of-ideological-placement-in-parliamentary-corpora%2F017F0CEA9B3DB6E1B94AC36A509A8A7B&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWord2vec" class="Z3988"></span></span>
</li>
<li id="cite_note-14"><span class="mw-cite-backlink"><b><a href="#cite_ref-14">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFNay2017" class="citation journal cs1">Nay, John (21 December 2017). <a rel="nofollow" class="external text" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3087278">"Gov2Vec: Learning Distributed Representations of Institutions and Their Legal Text"</a>. <i>SSRN</i>. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1609.06616">1609.06616</a></span>. <a href="/wiki/SSRN_(identifier)" class="mw-redirect" title="SSRN (identifier)">SSRN</a>&#160;<a rel="nofollow" class="external text" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3087278">3087278</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=SSRN&amp;rft.atitle=Gov2Vec%3A+Learning+Distributed+Representations+of+Institutions+and+Their+Legal+Text&amp;rft.date=2017-12-21&amp;rft_id=info%3Aarxiv%2F1609.06616&amp;rft_id=https%3A%2F%2Fpapers.ssrn.com%2Fsol3%2Fpapers.cfm%3Fabstract_id%3D3087278%23id-name%3DSSRN&amp;rft.aulast=Nay&amp;rft.aufirst=John&amp;rft_id=https%3A%2F%2Fpapers.ssrn.com%2Fsol3%2Fpapers.cfm%3Fabstract_id%3D3087278&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWord2vec" class="Z3988"></span></span>
</li>
<li id="cite_note-:3-15"><span class="mw-cite-backlink">^ <a href="#cite_ref-:3_15-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:3_15-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFAngelov2020" class="citation arxiv cs1">Angelov, Dimo (August 2020). "Top2Vec: Distributed Representations of Topics". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2008.09470">2008.09470</a></span> [<a rel="nofollow" class="external text" href="https://arxiv.org/archive/cs.CL">cs.CL</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Top2Vec%3A+Distributed+Representations+of+Topics&amp;rft.date=2020-08&amp;rft_id=info%3Aarxiv%2F2008.09470&amp;rft.aulast=Angelov&amp;rft.aufirst=Dimo&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWord2vec" class="Z3988"></span></span>
</li>
<li id="cite_note-16"><span class="mw-cite-backlink"><b><a href="#cite_ref-16">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFAngelov2022" class="citation web cs1">Angelov, Dimo (11 November 2022). <a rel="nofollow" class="external text" href="https://github.com/ddangelov/Top2Vec">"Top2Vec"</a>. <i><a href="/wiki/GitHub" title="GitHub">GitHub</a></i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=GitHub&amp;rft.atitle=Top2Vec&amp;rft.date=2022-11-11&amp;rft.aulast=Angelov&amp;rft.aufirst=Dimo&amp;rft_id=https%3A%2F%2Fgithub.com%2Fddangelov%2FTop2Vec&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWord2vec" class="Z3988"></span></span>
</li>
<li id="cite_note-17"><span class="mw-cite-backlink"><b><a href="#cite_ref-17">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFCampelloMoulaviSander2013" class="citation book cs1">Campello, Ricardo; Moulavi, Davoud; Sander, Joerg (2013). <a rel="nofollow" class="external text" href="https://link.springer.com/chapter/10.1007/978-3-642-37456-2_14">"Density-Based Clustering Based on Hierarchical Density Estimates"</a>. <i>Advances in Knowledge Discovery and Data Mining</i>. Lecture Notes in Computer Science. Vol.&#160;7819. pp.&#160;160–172. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1007%2F978-3-642-37456-2_14">10.1007/978-3-642-37456-2_14</a>. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-3-642-37455-5" title="Special:BookSources/978-3-642-37455-5"><bdi>978-3-642-37455-5</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Density-Based+Clustering+Based+on+Hierarchical+Density+Estimates&amp;rft.btitle=Advances+in+Knowledge+Discovery+and+Data+Mining&amp;rft.series=Lecture+Notes+in+Computer+Science&amp;rft.pages=160-172&amp;rft.date=2013&amp;rft_id=info%3Adoi%2F10.1007%2F978-3-642-37456-2_14&amp;rft.isbn=978-3-642-37455-5&amp;rft.aulast=Campello&amp;rft.aufirst=Ricardo&amp;rft.au=Moulavi%2C+Davoud&amp;rft.au=Sander%2C+Joerg&amp;rft_id=https%3A%2F%2Flink.springer.com%2Fchapter%2F10.1007%2F978-3-642-37456-2_14&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWord2vec" class="Z3988"></span></span>
</li>
<li id="cite_note-:0-18"><span class="mw-cite-backlink">^ <a href="#cite_ref-:0_18-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:0_18-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFAsgariMofrad2015" class="citation journal cs1">Asgari, Ehsaneddin; Mofrad, Mohammad R.K. (2015). <a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4640716">"Continuous Distributed Representation of Biological Sequences for Deep Proteomics and Genomics"</a>. <i>PLOS ONE</i>. <b>10</b> (11): e0141287. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1503.05140">1503.05140</a></span>. <a href="/wiki/Bibcode_(identifier)" class="mw-redirect" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/2015PLoSO..1041287A">2015PLoSO..1041287A</a>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1371%2Fjournal.pone.0141287">10.1371/journal.pone.0141287</a></span>. <a href="/wiki/PMC_(identifier)" class="mw-redirect" title="PMC (identifier)">PMC</a>&#160;<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4640716">4640716</a></span>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/26555596">26555596</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=PLOS+ONE&amp;rft.atitle=Continuous+Distributed+Representation+of+Biological+Sequences+for+Deep+Proteomics+and+Genomics&amp;rft.volume=10&amp;rft.issue=11&amp;rft.pages=e0141287&amp;rft.date=2015&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC4640716%23id-name%3DPMC&amp;rft_id=info%3Abibcode%2F2015PLoSO..1041287A&amp;rft_id=info%3Aarxiv%2F1503.05140&amp;rft_id=info%3Apmid%2F26555596&amp;rft_id=info%3Adoi%2F10.1371%2Fjournal.pone.0141287&amp;rft.aulast=Asgari&amp;rft.aufirst=Ehsaneddin&amp;rft.au=Mofrad%2C+Mohammad+R.K.&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC4640716&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWord2vec" class="Z3988"></span></span>
</li>
<li id="cite_note-19"><span class="mw-cite-backlink"><b><a href="#cite_ref-19">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFNg2017" class="citation arxiv cs1">Ng, Patrick (2017). "dna2vec: Consistent vector representations of variable-length k-mers". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1701.06279">1701.06279</a></span> [<a rel="nofollow" class="external text" href="https://arxiv.org/archive/q-bio.QM">q-bio.QM</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=dna2vec%3A+Consistent+vector+representations+of+variable-length+k-mers&amp;rft.date=2017&amp;rft_id=info%3Aarxiv%2F1701.06279&amp;rft.aulast=Ng&amp;rft.aufirst=Patrick&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWord2vec" class="Z3988"></span></span>
</li>
<li id="cite_note-20"><span class="mw-cite-backlink"><b><a href="#cite_ref-20">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFBanerjeeChenLungrenRubin2018" class="citation journal cs1">Banerjee, Imon; Chen, Matthew C.; Lungren, Matthew P.; Rubin, Daniel L. (2018). <a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5771955">"Radiology report annotation using intelligent word embeddings: Applied to multi-institutional chest CT cohort"</a>. <i>Journal of Biomedical Informatics</i>. <b>77</b>: 11–20. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.jbi.2017.11.012">10.1016/j.jbi.2017.11.012</a>. <a href="/wiki/PMC_(identifier)" class="mw-redirect" title="PMC (identifier)">PMC</a>&#160;<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5771955">5771955</a></span>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/29175548">29175548</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Biomedical+Informatics&amp;rft.atitle=Radiology+report+annotation+using+intelligent+word+embeddings%3A+Applied+to+multi-institutional+chest+CT+cohort&amp;rft.volume=77&amp;rft.pages=11-20&amp;rft.date=2018&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC5771955%23id-name%3DPMC&amp;rft_id=info%3Apmid%2F29175548&amp;rft_id=info%3Adoi%2F10.1016%2Fj.jbi.2017.11.012&amp;rft.aulast=Banerjee&amp;rft.aufirst=Imon&amp;rft.au=Chen%2C+Matthew+C.&amp;rft.au=Lungren%2C+Matthew+P.&amp;rft.au=Rubin%2C+Daniel+L.&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC5771955&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWord2vec" class="Z3988"></span></span>
</li>
<li id="cite_note-21"><span class="mw-cite-backlink"><b><a href="#cite_ref-21">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFLevyGoldbergDagan2015" class="citation journal cs1">Levy, Omer; Goldberg, Yoav; Dagan, Ido (2015). <a rel="nofollow" class="external text" href="http://www.aclweb.org/anthology/Q15-1016">"Improving Distributional Similarity with Lessons Learned from Word Embeddings"</a>. <i>Transactions of the Association for Computational Linguistics</i>. <b>3</b>. Transactions of the Association for Computational Linguistics: 211–225. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1162%2Ftacl_a_00134">10.1162/tacl_a_00134</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Transactions+of+the+Association+for+Computational+Linguistics&amp;rft.atitle=Improving+Distributional+Similarity+with+Lessons+Learned+from+Word+Embeddings&amp;rft.volume=3&amp;rft.pages=211-225&amp;rft.date=2015&amp;rft_id=info%3Adoi%2F10.1162%2Ftacl_a_00134&amp;rft.aulast=Levy&amp;rft.aufirst=Omer&amp;rft.au=Goldberg%2C+Yoav&amp;rft.au=Dagan%2C+Ido&amp;rft_id=http%3A%2F%2Fwww.aclweb.org%2Fanthology%2FQ15-1016&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWord2vec" class="Z3988"></span></span>
</li>
<li id="cite_note-22"><span class="mw-cite-backlink"><b><a href="#cite_ref-22">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFArora2016" class="citation journal cs1">Arora, S; et&#160;al. (Summer 2016). <a rel="nofollow" class="external text" href="http://aclweb.org/anthology/Q16-1028">"A Latent Variable Model Approach to PMI-based Word Embeddings"</a>. <i>Transactions of the Association for Computational Linguistics</i>. <b>4</b>: 385–399. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1502.03520">1502.03520</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1162%2Ftacl_a_00106">10.1162/tacl_a_00106</a></span> &#8211; via ACLWEB.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Transactions+of+the+Association+for+Computational+Linguistics&amp;rft.atitle=A+Latent+Variable+Model+Approach+to+PMI-based+Word+Embeddings&amp;rft.ssn=summer&amp;rft.volume=4&amp;rft.pages=385-399&amp;rft.date=2016&amp;rft_id=info%3Aarxiv%2F1502.03520&amp;rft_id=info%3Adoi%2F10.1162%2Ftacl_a_00106&amp;rft.aulast=Arora&amp;rft.aufirst=S&amp;rft_id=http%3A%2F%2Faclweb.org%2Fanthology%2FQ16-1028&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWord2vec" class="Z3988"></span></span>
</li>
<li id="cite_note-23"><span class="mw-cite-backlink"><b><a href="#cite_ref-23">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFMikolovYihZweig2013" class="citation journal cs1">Mikolov, Tomas; Yih, Wen-tau; Zweig, Geoffrey (2013). "Linguistic Regularities in Continuous Space Word Representations". <i>HLT-Naacl</i>: 746–751.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=HLT-Naacl&amp;rft.atitle=Linguistic+Regularities+in+Continuous+Space+Word+Representations.&amp;rft.pages=746-751&amp;rft.date=2013&amp;rft.aulast=Mikolov&amp;rft.aufirst=Tomas&amp;rft.au=Yih%2C+Wen-tau&amp;rft.au=Zweig%2C+Geoffrey&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWord2vec" class="Z3988"></span></span>
</li>
<li id="cite_note-24"><span class="mw-cite-backlink"><b><a href="#cite_ref-24">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFJansen2017" class="citation journal cs1">Jansen, Stefan (9 May 2017). "Word and Phrase Translation with word2vec". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1705.03127">1705.03127</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Word+and+Phrase+Translation+with+word2vec&amp;rft.date=2017-05-09&amp;rft_id=info%3Aarxiv%2F1705.03127&amp;rft.aulast=Jansen&amp;rft.aufirst=Stefan&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWord2vec" class="Z3988"></span> <span class="cs1-visible-error citation-comment"><code class="cs1-code">{{<a href="/wiki/Template:Cite_journal" title="Template:Cite journal">cite journal</a>}}</code>: </span><span class="cs1-visible-error citation-comment">Cite journal requires <code class="cs1-code">&#124;journal=</code> (<a href="/wiki/Help:CS1_errors#missing_periodical" title="Help:CS1 errors">help</a>)</span></span>
</li>
<li id="cite_note-25"><span class="mw-cite-backlink"><b><a href="#cite_ref-25">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://radimrehurek.com/gensim/models/word2vec.html">"Gensim - Deep learning with word2vec"</a><span class="reference-accessdate">. Retrieved <span class="nowrap">10 June</span> 2016</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Gensim+-+Deep+learning+with+word2vec&amp;rft_id=https%3A%2F%2Fradimrehurek.com%2Fgensim%2Fmodels%2Fword2vec.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWord2vec" class="Z3988"></span></span>
</li>
<li id="cite_note-Altszyler-26"><span class="mw-cite-backlink"><b><a href="#cite_ref-Altszyler_26-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFAltszyler,_E.Ribeiro,_S.Sigman,_M.Fernández_Slezak,_D.2017" class="citation journal cs1">Altszyler, E.; Ribeiro, S.; Sigman, M.; Fernández Slezak, D. (2017). "The interpretation of dream meaning: Resolving ambiguity using Latent Semantic Analysis in a small corpus of text". <i>Consciousness and Cognition</i>. <b>56</b>: 178–187. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1610.01520">1610.01520</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.concog.2017.09.004">10.1016/j.concog.2017.09.004</a>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/28943127">28943127</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:195347873">195347873</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Consciousness+and+Cognition&amp;rft.atitle=The+interpretation+of+dream+meaning%3A+Resolving+ambiguity+using+Latent+Semantic+Analysis+in+a+small+corpus+of+text&amp;rft.volume=56&amp;rft.pages=178-187&amp;rft.date=2017&amp;rft_id=info%3Aarxiv%2F1610.01520&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A195347873%23id-name%3DS2CID&amp;rft_id=info%3Apmid%2F28943127&amp;rft_id=info%3Adoi%2F10.1016%2Fj.concog.2017.09.004&amp;rft.au=Altszyler%2C+E.&amp;rft.au=Ribeiro%2C+S.&amp;rft.au=Sigman%2C+M.&amp;rft.au=Fern%C3%A1ndez+Slezak%2C+D.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AWord2vec" class="Z3988"></span></span>
</li>
</ol></div></div>
<h2><span class="mw-headline" id="External_links">External links</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Word2vec&amp;action=edit&amp;section=22" title="Edit section: External links"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h2>
<ul><li><a rel="nofollow" class="external text" href="https://wikipedia2vec.github.io/demo/">Wikipedia2Vec</a><a rel="nofollow" class="external autonumber" href="http://arxiv.org/abs/1812.06280">[1]</a> (<a rel="nofollow" class="external text" href="https://wikipedia2vec.github.io/wikipedia2vec/">introduction</a>)</li></ul>
<h3><span class="mw-headline" id="Implementations">Implementations</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Word2vec&amp;action=edit&amp;section=23" title="Edit section: Implementations"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h3>
<ul><li><a rel="nofollow" class="external text" href="https://github.com/tmikolov/word2vec">C</a></li>
<li><a rel="nofollow" class="external text" href="https://github.com/eabdullin/Word2Vec.Net">C#</a></li>
<li><a rel="nofollow" class="external text" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.Word2Vec.html">Python (Spark)</a></li>
<li><a rel="nofollow" class="external text" href="https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/examples/tutorials/word2vec/word2vec_basic.py">Python (TensorFlow)</a></li>
<li><a rel="nofollow" class="external text" href="http://radimrehurek.com/gensim/models/word2vec.html">Python (Gensim)</a></li>
<li><a rel="nofollow" class="external text" href="https://github.com/deeplearning4j/deeplearning4j">Java/Scala</a></li>
<li><a rel="nofollow" class="external text" href="https://github.com/bnosac/word2vec">R</a></li></ul>
<div class="navbox-styles"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1129693374"><style data-mw-deduplicate="TemplateStyles:r1061467846">.mw-parser-output .navbox{box-sizing:border-box;border:1px solid #a2a9b1;width:100%;clear:both;font-size:88%;text-align:center;padding:1px;margin:1em auto 0}.mw-parser-output .navbox .navbox{margin-top:0}.mw-parser-output .navbox+.navbox,.mw-parser-output .navbox+.navbox-styles+.navbox{margin-top:-1px}.mw-parser-output .navbox-inner,.mw-parser-output .navbox-subgroup{width:100%}.mw-parser-output .navbox-group,.mw-parser-output .navbox-title,.mw-parser-output .navbox-abovebelow{padding:0.25em 1em;line-height:1.5em;text-align:center}.mw-parser-output .navbox-group{white-space:nowrap;text-align:right}.mw-parser-output .navbox,.mw-parser-output .navbox-subgroup{background-color:#fdfdfd}.mw-parser-output .navbox-list{line-height:1.5em;border-color:#fdfdfd}.mw-parser-output .navbox-list-with-group{text-align:left;border-left-width:2px;border-left-style:solid}.mw-parser-output tr+tr>.navbox-abovebelow,.mw-parser-output tr+tr>.navbox-group,.mw-parser-output tr+tr>.navbox-image,.mw-parser-output tr+tr>.navbox-list{border-top:2px solid #fdfdfd}.mw-parser-output .navbox-title{background-color:#ccf}.mw-parser-output .navbox-abovebelow,.mw-parser-output .navbox-group,.mw-parser-output .navbox-subgroup .navbox-title{background-color:#ddf}.mw-parser-output .navbox-subgroup .navbox-group,.mw-parser-output .navbox-subgroup .navbox-abovebelow{background-color:#e6e6ff}.mw-parser-output .navbox-even{background-color:#f7f7f7}.mw-parser-output .navbox-odd{background-color:transparent}.mw-parser-output .navbox .hlist td dl,.mw-parser-output .navbox .hlist td ol,.mw-parser-output .navbox .hlist td ul,.mw-parser-output .navbox td.hlist dl,.mw-parser-output .navbox td.hlist ol,.mw-parser-output .navbox td.hlist ul{padding:0.125em 0}.mw-parser-output .navbox .navbar{display:block;font-size:100%}.mw-parser-output .navbox-title .navbar{float:left;text-align:left;margin-right:0.5em}</style></div><div role="navigation" class="navbox" aria-labelledby="Natural_language_processing" style="padding:3px"><table class="nowraplinks hlist mw-collapsible autocollapse navbox-inner" style="border-spacing:0;background:transparent;color:inherit"><tbody><tr><th scope="col" class="navbox-title" colspan="2"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1129693374"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1063604349"><div class="navbar plainlinks hlist navbar-mini"><ul><li class="nv-view"><a href="/wiki/Template:Natural_language_processing" title="Template:Natural language processing"><abbr title="View this template" style=";;background:none transparent;border:none;box-shadow:none;padding:0;">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Natural_language_processing" title="Template talk:Natural language processing"><abbr title="Discuss this template" style=";;background:none transparent;border:none;box-shadow:none;padding:0;">t</abbr></a></li><li class="nv-edit"><a href="/wiki/Special:EditPage/Template:Natural_language_processing" title="Special:EditPage/Template:Natural language processing"><abbr title="Edit this template" style=";;background:none transparent;border:none;box-shadow:none;padding:0;">e</abbr></a></li></ul></div><div id="Natural_language_processing" style="font-size:114%;margin:0 4em"><a href="/wiki/Natural_language_processing" title="Natural language processing">Natural language processing</a></div></th></tr><tr><th scope="row" class="navbox-group" style="width:1%">General terms</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/AI-complete" title="AI-complete">AI-complete</a></li>
<li><a href="/wiki/Bag-of-words_model" title="Bag-of-words model">Bag-of-words</a></li>
<li><a href="/wiki/N-gram" title="N-gram">n-gram</a>
<ul><li><a href="/wiki/Bigram" title="Bigram">Bigram</a></li>
<li><a href="/wiki/Trigram" title="Trigram">Trigram</a></li></ul></li>
<li><a href="/wiki/Computational_linguistics" title="Computational linguistics">Computational linguistics</a></li>
<li><a href="/wiki/Natural-language_understanding" title="Natural-language understanding">Natural-language understanding</a></li>
<li><a href="/wiki/Stop_word" title="Stop word">Stop words</a></li>
<li><a href="/wiki/Text_processing" title="Text processing">Text processing</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Text_mining" title="Text mining">Text analysis</a></th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Argument_mining" title="Argument mining">Argument mining</a></li>
<li><a href="/wiki/Collocation_extraction" title="Collocation extraction">Collocation extraction</a></li>
<li><a href="/wiki/Concept_mining" title="Concept mining">Concept mining</a></li>
<li><a href="/wiki/Coreference#Coreference_resolution" title="Coreference">Coreference resolution</a></li>
<li><a href="/wiki/Deep_linguistic_processing" title="Deep linguistic processing">Deep linguistic processing</a></li>
<li><a href="/wiki/Distant_reading" title="Distant reading">Distant reading</a></li>
<li><a href="/wiki/Information_extraction" title="Information extraction">Information extraction</a></li>
<li><a href="/wiki/Named-entity_recognition" title="Named-entity recognition">Named-entity recognition</a></li>
<li><a href="/wiki/Ontology_learning" title="Ontology learning">Ontology learning</a></li>
<li><a href="/wiki/Parsing" title="Parsing">Parsing</a>
<ul><li><a href="/wiki/Semantic_parsing" title="Semantic parsing">Semantic parsing</a></li>
<li><a href="/wiki/Syntactic_parsing_(computational_linguistics)" title="Syntactic parsing (computational linguistics)">Syntactic parsing</a></li></ul></li>
<li><a href="/wiki/Part-of-speech_tagging" title="Part-of-speech tagging">Part-of-speech tagging</a></li>
<li><a href="/wiki/Semantic_analysis_(machine_learning)" title="Semantic analysis (machine learning)">Semantic analysis</a></li>
<li><a href="/wiki/Semantic_role_labeling" title="Semantic role labeling">Semantic role labeling</a></li>
<li><a href="/wiki/Semantic_decomposition_(natural_language_processing)" title="Semantic decomposition (natural language processing)">Semantic decomposition</a></li>
<li><a href="/wiki/Semantic_similarity" title="Semantic similarity">Semantic similarity</a></li>
<li><a href="/wiki/Sentiment_analysis" title="Sentiment analysis">Sentiment analysis</a></li></ul>
<ul><li><a href="/wiki/Terminology_extraction" title="Terminology extraction">Terminology extraction</a></li>
<li><a href="/wiki/Text_mining" title="Text mining">Text mining</a></li>
<li><a href="/wiki/Textual_entailment" title="Textual entailment">Textual entailment</a></li>
<li><a href="/wiki/Truecasing" title="Truecasing">Truecasing</a></li>
<li><a href="/wiki/Word-sense_disambiguation" title="Word-sense disambiguation">Word-sense disambiguation</a></li>
<li><a href="/wiki/Word-sense_induction" title="Word-sense induction">Word-sense induction</a></li></ul>
</div><table class="nowraplinks navbox-subgroup" style="border-spacing:0"><tbody><tr><th id="Text_segmentation" scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Text_segmentation" title="Text segmentation">Text segmentation</a></th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Compound-term_processing" title="Compound-term processing">Compound-term processing</a></li>
<li><a href="/wiki/Lemmatisation" class="mw-redirect" title="Lemmatisation">Lemmatisation</a></li>
<li><a href="/wiki/Lexical_analysis" title="Lexical analysis">Lexical analysis</a></li>
<li><a href="/wiki/Shallow_parsing" title="Shallow parsing">Text chunking</a></li>
<li><a href="/wiki/Stemming" title="Stemming">Stemming</a></li>
<li><a href="/wiki/Sentence_boundary_disambiguation" title="Sentence boundary disambiguation">Sentence segmentation</a></li>
<li><a href="/wiki/Word#Word_boundaries" title="Word">Word segmentation</a></li></ul>
</div></td></tr></tbody></table><div>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Automatic_summarization" title="Automatic summarization">Automatic summarization</a></th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Multi-document_summarization" title="Multi-document summarization">Multi-document summarization</a></li>
<li><a href="/wiki/Sentence_extraction" title="Sentence extraction">Sentence extraction</a></li>
<li><a href="/wiki/Text_simplification" title="Text simplification">Text simplification</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Machine_translation" title="Machine translation">Machine translation</a></th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Computer-assisted_translation" title="Computer-assisted translation">Computer-assisted</a></li>
<li><a href="/wiki/Example-based_machine_translation" title="Example-based machine translation">Example-based</a></li>
<li><a href="/wiki/Rule-based_machine_translation" title="Rule-based machine translation">Rule-based</a></li>
<li><a href="/wiki/Statistical_machine_translation" title="Statistical machine translation">Statistical</a></li>
<li><a href="/wiki/Transfer-based_machine_translation" title="Transfer-based machine translation">Transfer-based</a></li>
<li><a href="/wiki/Neural_machine_translation" title="Neural machine translation">Neural</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Distributional_semantics" title="Distributional semantics">Distributional semantics</a> models</th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/BERT_(language_model)" title="BERT (language model)">BERT</a></li>
<li><a href="/wiki/Document-term_matrix" title="Document-term matrix">Document-term matrix</a></li>
<li><a href="/wiki/Explicit_semantic_analysis" title="Explicit semantic analysis">Explicit semantic analysis</a></li>
<li><a href="/wiki/FastText" title="FastText">fastText</a></li>
<li><a href="/wiki/GloVe" title="GloVe">GloVe</a></li>
<li><a href="/wiki/Language_model" title="Language model">Language model</a> (<a href="/wiki/Large_language_model" title="Large language model">large</a>)</li>
<li><a href="/wiki/Latent_semantic_analysis" title="Latent semantic analysis">Latent semantic analysis</a></li>
<li><a href="/wiki/Seq2seq" title="Seq2seq">Seq2seq</a></li>
<li><a href="/wiki/Word_embedding" title="Word embedding">Word embedding</a></li>
<li><a class="mw-selflink selflink">Word2vec</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Language_resource" title="Language resource">Language resources</a>,<br />datasets and corpora</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em"></div><table class="nowraplinks navbox-subgroup" style="border-spacing:0"><tbody><tr><th scope="row" class="navbox-group" style="width:1%">Types and<br />standards</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Corpus_linguistics" title="Corpus linguistics">Corpus linguistics</a></li>
<li><a href="/wiki/Lexical_resource" title="Lexical resource">Lexical resource</a></li>
<li><a href="/wiki/Linguistic_Linked_Open_Data" title="Linguistic Linked Open Data">Linguistic Linked Open Data</a></li>
<li><a href="/wiki/Machine-readable_dictionary" title="Machine-readable dictionary">Machine-readable dictionary</a></li>
<li><a href="/wiki/Parallel_text" title="Parallel text">Parallel text</a></li>
<li><a href="/wiki/PropBank" title="PropBank">PropBank</a></li>
<li><a href="/wiki/Semantic_network" title="Semantic network">Semantic network</a></li>
<li><a href="/wiki/Simple_Knowledge_Organization_System" title="Simple Knowledge Organization System">Simple Knowledge Organization System</a></li>
<li><a href="/wiki/Speech_corpus" title="Speech corpus">Speech corpus</a></li>
<li><a href="/wiki/Text_corpus" title="Text corpus">Text corpus</a></li>
<li><a href="/wiki/Thesaurus_(information_retrieval)" title="Thesaurus (information retrieval)">Thesaurus (information retrieval)</a></li>
<li><a href="/wiki/Treebank" title="Treebank">Treebank</a></li>
<li><a href="/wiki/Universal_Dependencies" title="Universal Dependencies">Universal Dependencies</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Data</th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/BabelNet" title="BabelNet">BabelNet</a></li>
<li><a href="/wiki/Bank_of_English" title="Bank of English">Bank of English</a></li>
<li><a href="/wiki/DBpedia" title="DBpedia">DBpedia</a></li>
<li><a href="/wiki/FrameNet" title="FrameNet">FrameNet</a></li>
<li><a href="/wiki/Google_Ngram_Viewer" title="Google Ngram Viewer">Google Ngram Viewer</a></li>
<li><a href="/wiki/UBY" title="UBY">UBY</a></li>
<li><a href="/wiki/WordNet" title="WordNet">WordNet</a></li></ul>
</div></td></tr></tbody></table><div></div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Automatic_identification_and_data_capture" title="Automatic identification and data capture">Automatic identification<br />and data capture</a></th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Speech_recognition" title="Speech recognition">Speech recognition</a></li>
<li><a href="/wiki/Speech_segmentation" title="Speech segmentation">Speech segmentation</a></li>
<li><a href="/wiki/Speech_synthesis" title="Speech synthesis">Speech synthesis</a></li>
<li><a href="/wiki/Natural_language_generation" title="Natural language generation">Natural language generation</a></li>
<li><a href="/wiki/Optical_character_recognition" title="Optical character recognition">Optical character recognition</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Topic_model" title="Topic model">Topic model</a></th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Document_classification" title="Document classification">Document classification</a></li>
<li><a href="/wiki/Latent_Dirichlet_allocation" title="Latent Dirichlet allocation">Latent Dirichlet allocation</a></li>
<li><a href="/wiki/Pachinko_allocation" title="Pachinko allocation">Pachinko allocation</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Computer-assisted_reviewing" title="Computer-assisted reviewing">Computer-assisted<br />reviewing</a></th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Automated_essay_scoring" title="Automated essay scoring">Automated essay scoring</a></li>
<li><a href="/wiki/Concordancer" title="Concordancer">Concordancer</a></li>
<li><a href="/wiki/Grammar_checker" title="Grammar checker">Grammar checker</a></li>
<li><a href="/wiki/Predictive_text" title="Predictive text">Predictive text</a></li>
<li><a href="/wiki/Pronunciation_assessment" title="Pronunciation assessment">Pronunciation assessment</a></li>
<li><a href="/wiki/Spell_checker" title="Spell checker">Spell checker</a></li>
<li><a href="/wiki/Syntax_guessing" class="mw-redirect" title="Syntax guessing">Syntax guessing</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Natural_language_user_interface" class="mw-redirect" title="Natural language user interface">Natural language<br />user interface</a></th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Chatbot" title="Chatbot">Chatbot</a></li>
<li><a href="/wiki/Interactive_fiction" title="Interactive fiction">Interactive fiction</a></li>
<li><a href="/wiki/Question_answering" title="Question answering">Question answering</a></li>
<li><a href="/wiki/Virtual_assistant" title="Virtual assistant">Virtual assistant</a></li>
<li><a href="/wiki/Voice_user_interface" title="Voice user interface">Voice user interface</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Related</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Formal_semantics_(natural_language)" title="Formal semantics (natural language)">Formal semantics</a></li>
<li><a href="/wiki/Hallucination_(artificial_intelligence)" title="Hallucination (artificial intelligence)">Hallucination</a></li>
<li><a href="/wiki/Natural_Language_Toolkit" title="Natural Language Toolkit">Natural Language Toolkit</a></li>
<li><a href="/wiki/SpaCy" title="SpaCy">spaCy</a></li></ul>
</div></td></tr></tbody></table></div>
<div class="navbox-styles"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1129693374"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1061467846"></div><div role="navigation" class="navbox" aria-labelledby="Differentiable_computing" style="padding:3px"><table class="nowraplinks hlist mw-collapsible autocollapse navbox-inner" style="border-spacing:0;background:transparent;color:inherit"><tbody><tr><th scope="col" class="navbox-title" colspan="2"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1129693374"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1063604349"><div class="navbar plainlinks hlist navbar-mini"><ul><li class="nv-view"><a href="/wiki/Template:Differentiable_computing" title="Template:Differentiable computing"><abbr title="View this template" style=";;background:none transparent;border:none;box-shadow:none;padding:0;">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Differentiable_computing" title="Template talk:Differentiable computing"><abbr title="Discuss this template" style=";;background:none transparent;border:none;box-shadow:none;padding:0;">t</abbr></a></li><li class="nv-edit"><a href="/wiki/Special:EditPage/Template:Differentiable_computing" title="Special:EditPage/Template:Differentiable computing"><abbr title="Edit this template" style=";;background:none transparent;border:none;box-shadow:none;padding:0;">e</abbr></a></li></ul></div><div id="Differentiable_computing" style="font-size:114%;margin:0 4em">Differentiable computing</div></th></tr><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Differentiable_function" title="Differentiable function">General</a></th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><b><a href="/wiki/Differentiable_programming" title="Differentiable programming">Differentiable programming</a></b></li>
<li><a href="/wiki/Information_geometry" title="Information geometry">Information geometry</a></li>
<li><a href="/wiki/Statistical_manifold" title="Statistical manifold">Statistical manifold</a></li>
<li><a href="/wiki/Automatic_differentiation" title="Automatic differentiation">Automatic differentiation</a></li>
<li><a href="/wiki/Neuromorphic_engineering" title="Neuromorphic engineering">Neuromorphic engineering</a></li>
<li><a href="/wiki/Pattern_recognition" title="Pattern recognition">Pattern recognition</a></li>
<li><a href="/wiki/Tensor_calculus" title="Tensor calculus">Tensor calculus</a></li>
<li><a href="/wiki/Computational_learning_theory" title="Computational learning theory">Computational learning theory</a></li>
<li><a href="/wiki/Inductive_bias" title="Inductive bias">Inductive bias</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Concepts</th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Gradient_descent" title="Gradient descent">Gradient descent</a>
<ul><li><a href="/wiki/Stochastic_gradient_descent" title="Stochastic gradient descent">SGD</a></li></ul></li>
<li><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></li>
<li><a href="/wiki/Regression_analysis" title="Regression analysis">Regression</a>
<ul><li><a href="/wiki/Overfitting" title="Overfitting">Overfitting</a></li></ul></li>
<li><a href="/wiki/Hallucination_(artificial_intelligence)" title="Hallucination (artificial intelligence)">Hallucination</a></li>
<li><a href="/wiki/Adversarial_machine_learning" title="Adversarial machine learning">Adversary</a></li>
<li><a href="/wiki/Attention_(machine_learning)" title="Attention (machine learning)">Attention</a></li>
<li><a href="/wiki/Convolution" title="Convolution">Convolution</a></li>
<li><a href="/wiki/Loss_functions_for_classification" title="Loss functions for classification">Loss functions</a></li>
<li><a href="/wiki/Backpropagation" title="Backpropagation">Backpropagation</a></li>
<li><a href="/wiki/Batch_normalization" title="Batch normalization">Batchnorm</a></li>
<li><a href="/wiki/Activation_function" title="Activation function">Activation</a>
<ul><li><a href="/wiki/Softmax_function" title="Softmax function">Softmax</a></li>
<li><a href="/wiki/Sigmoid_function" title="Sigmoid function">Sigmoid</a></li>
<li><a href="/wiki/Rectifier_(neural_networks)" title="Rectifier (neural networks)">Rectifier</a></li></ul></li>
<li><a href="/wiki/Regularization_(mathematics)" title="Regularization (mathematics)">Regularization</a></li>
<li><a href="/wiki/Training,_validation,_and_test_sets" class="mw-redirect" title="Training, validation, and test sets">Datasets</a>
<ul><li><a href="/wiki/Data_augmentation" title="Data augmentation">Augmentation</a></li></ul></li>
<li><a href="/wiki/Diffusion_process" title="Diffusion process">Diffusion</a></li>
<li><a href="/wiki/Autoregressive_model" title="Autoregressive model">Autoregression</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Applications</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a>
<ul><li><a href="/wiki/Prompt_engineering#In-context_learning" title="Prompt engineering">In-context learning</a></li></ul></li>
<li><a href="/wiki/Artificial_neural_network" class="mw-redirect" title="Artificial neural network">Artificial neural network</a>
<ul><li><a href="/wiki/Deep_learning" title="Deep learning">Deep learning</a></li></ul></li>
<li><a href="/wiki/Computational_science" title="Computational science">Scientific computing</a></li>
<li><a href="/wiki/Artificial_intelligence" title="Artificial intelligence">Artificial Intelligence</a></li>
<li><a href="/wiki/Language_model" title="Language model">Language model</a>
<ul><li><a href="/wiki/Large_language_model" title="Large language model">Large language model</a></li></ul></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Hardware</th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Graphcore" title="Graphcore">IPU</a></li>
<li><a href="/wiki/Tensor_Processing_Unit" title="Tensor Processing Unit">TPU</a></li>
<li><a href="/wiki/Vision_processing_unit" title="Vision processing unit">VPU</a></li>
<li><a href="/wiki/Memristor" title="Memristor">Memristor</a></li>
<li><a href="/wiki/SpiNNaker" title="SpiNNaker">SpiNNaker</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Software libraries</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/TensorFlow" title="TensorFlow">TensorFlow</a></li>
<li><a href="/wiki/PyTorch" title="PyTorch">PyTorch</a></li>
<li><a href="/wiki/Keras" title="Keras">Keras</a></li>
<li><a href="/wiki/Theano_(software)" title="Theano (software)">Theano</a></li>
<li><a href="/wiki/Google_JAX" title="Google JAX">JAX</a></li>
<li><a href="/wiki/Flux_(machine-learning_framework)" title="Flux (machine-learning framework)">Flux.jl</a></li>
<li><a href="/wiki/MindSpore" title="MindSpore">MindSpore</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Implementations</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em"></div><table class="nowraplinks navbox-subgroup" style="border-spacing:0"><tbody><tr><th scope="row" class="navbox-group" style="width:1%">Audio–visual</th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/AlexNet" title="AlexNet">AlexNet</a></li>
<li><a href="/wiki/WaveNet" title="WaveNet">WaveNet</a></li>
<li><a href="/wiki/Human_image_synthesis" title="Human image synthesis">Human image synthesis</a></li>
<li><a href="/wiki/Handwriting_recognition" title="Handwriting recognition">HWR</a></li>
<li><a href="/wiki/Optical_character_recognition" title="Optical character recognition">OCR</a></li>
<li><a href="/wiki/Deep_learning_speech_synthesis" title="Deep learning speech synthesis">Speech synthesis</a></li>
<li><a href="/wiki/Speech_recognition" title="Speech recognition">Speech recognition</a></li>
<li><a href="/wiki/Facial_recognition_system" title="Facial recognition system">Facial recognition</a></li>
<li><a href="/wiki/AlphaFold" title="AlphaFold">AlphaFold</a></li>
<li><a href="/wiki/Text-to-image_model" title="Text-to-image model">Text-to-image models</a>
<ul><li><a href="/wiki/DALL-E" title="DALL-E">DALL-E</a></li>
<li><a href="/wiki/Midjourney" title="Midjourney">Midjourney</a></li>
<li><a href="/wiki/Stable_Diffusion" title="Stable Diffusion">Stable Diffusion</a></li></ul></li>
<li><a href="/wiki/Text-to-video_model" title="Text-to-video model">Text-to-video models</a>
<ul><li><a href="/wiki/Sora_(text-to-video_model)" title="Sora (text-to-video model)">Sora</a></li>
<li><a href="/wiki/VideoPoet" title="VideoPoet">VideoPoet</a></li></ul></li>
<li><a href="/wiki/Whisper_(speech_recognition_system)" title="Whisper (speech recognition system)">Whisper</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Verbal</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a class="mw-selflink selflink">Word2vec</a></li>
<li><a href="/wiki/Seq2seq" title="Seq2seq">Seq2seq</a></li>
<li><a href="/wiki/BERT_(language_model)" title="BERT (language model)">BERT</a></li>
<li><a href="/wiki/Gemini_(language_model)" title="Gemini (language model)">Gemini</a></li>
<li><a href="/wiki/LaMDA" title="LaMDA">LaMDA</a>
<ul><li><a href="/wiki/Bard_(chatbot)" class="mw-redirect" title="Bard (chatbot)">Bard</a></li></ul></li>
<li><a href="/wiki/Neural_machine_translation" title="Neural machine translation">NMT</a></li>
<li><a href="/wiki/Project_Debater" title="Project Debater">Project Debater</a></li>
<li><a href="/wiki/IBM_Watson" title="IBM Watson">IBM Watson</a></li>
<li><a href="/wiki/IBM_Watsonx" title="IBM Watsonx">IBM Watsonx</a></li>
<li><a href="/wiki/IBM_Granite" title="IBM Granite">Granite</a></li>
<li><a href="/wiki/GPT-1" title="GPT-1">GPT-1</a></li>
<li><a href="/wiki/GPT-2" title="GPT-2">GPT-2</a></li>
<li><a href="/wiki/GPT-3" title="GPT-3">GPT-3</a></li>
<li><a href="/wiki/GPT-4" title="GPT-4">GPT-4</a></li>
<li><a href="/wiki/ChatGPT" title="ChatGPT">ChatGPT</a></li>
<li><a href="/wiki/GPT-J" title="GPT-J">GPT-J</a></li>
<li><a href="/wiki/Chinchilla_AI" class="mw-redirect" title="Chinchilla AI">Chinchilla AI</a></li>
<li><a href="/wiki/PaLM" title="PaLM">PaLM</a></li>
<li><a href="/wiki/BLOOM_(language_model)" title="BLOOM (language model)">BLOOM</a></li>
<li><a href="/wiki/LLaMA" title="LLaMA">LLaMA</a></li>
<li><a href="/wiki/Huawei_PanGu" title="Huawei PanGu">PanGu-Σ</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Decisional</th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/AlphaGo" title="AlphaGo">AlphaGo</a></li>
<li><a href="/wiki/AlphaZero" title="AlphaZero">AlphaZero</a></li>
<li><a href="/wiki/Q-learning" title="Q-learning">Q-learning</a></li>
<li><a href="/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action" title="State–action–reward–state–action">SARSA</a></li>
<li><a href="/wiki/OpenAI_Five" title="OpenAI Five">OpenAI Five</a></li>
<li><a href="/wiki/Self-driving_car" title="Self-driving car">Self-driving car</a></li>
<li><a href="/wiki/MuZero" title="MuZero">MuZero</a></li>
<li><a href="/wiki/Action_selection" title="Action selection">Action selection</a>
<ul><li><a href="/wiki/Auto-GPT" title="Auto-GPT">Auto-GPT</a></li></ul></li>
<li><a href="/wiki/Robot_control" title="Robot control">Robot control</a></li></ul>
</div></td></tr></tbody></table><div></div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">People</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Yoshua_Bengio" title="Yoshua Bengio">Yoshua Bengio</a></li>
<li><a href="/wiki/Alex_Graves_(computer_scientist)" title="Alex Graves (computer scientist)">Alex Graves</a></li>
<li><a href="/wiki/Ian_Goodfellow" title="Ian Goodfellow">Ian Goodfellow</a></li>
<li><a href="/wiki/Stephen_Grossberg" title="Stephen Grossberg">Stephen Grossberg</a></li>
<li><a href="/wiki/Demis_Hassabis" title="Demis Hassabis">Demis Hassabis</a></li>
<li><a href="/wiki/Geoffrey_Hinton" title="Geoffrey Hinton">Geoffrey Hinton</a></li>
<li><a href="/wiki/Yann_LeCun" title="Yann LeCun">Yann LeCun</a></li>
<li><a href="/wiki/Fei-Fei_Li" title="Fei-Fei Li">Fei-Fei Li</a></li>
<li><a href="/wiki/Andrew_Ng" title="Andrew Ng">Andrew Ng</a></li>
<li><a href="/wiki/J%C3%BCrgen_Schmidhuber" title="Jürgen Schmidhuber">Jürgen Schmidhuber</a></li>
<li><a href="/wiki/David_Silver_(computer_scientist)" title="David Silver (computer scientist)">David Silver</a></li>
<li><a href="/wiki/Ilya_Sutskever" title="Ilya Sutskever">Ilya Sutskever</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Organizations</th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Anthropic" title="Anthropic">Anthropic</a></li>
<li><a href="/wiki/EleutherAI" title="EleutherAI">EleutherAI</a></li>
<li><a href="/wiki/Google_DeepMind" title="Google DeepMind">Google DeepMind</a></li>
<li><a href="/wiki/Hugging_Face" title="Hugging Face">Hugging Face</a></li>
<li><a href="/wiki/OpenAI" title="OpenAI">OpenAI</a></li>
<li><a href="/wiki/Meta_AI" title="Meta AI">Meta AI</a></li>
<li><a href="/wiki/Mila_(research_institute)" title="Mila (research institute)">Mila</a></li>
<li><a href="/wiki/MIT_Computer_Science_and_Artificial_Intelligence_Laboratory" title="MIT Computer Science and Artificial Intelligence Laboratory">MIT CSAIL</a></li>
<li><a href="/wiki/Huawei" title="Huawei">Huawei</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Architectures</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Neural_Turing_machine" title="Neural Turing machine">Neural Turing machine</a></li>
<li><a href="/wiki/Differentiable_neural_computer" title="Differentiable neural computer">Differentiable neural computer</a></li>
<li><a href="/wiki/Transformer_(machine_learning_model)" class="mw-redirect" title="Transformer (machine learning model)">Transformer</a></li>
<li><a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">Recurrent neural network (RNN)</a></li>
<li><a href="/wiki/Long_short-term_memory" title="Long short-term memory">Long short-term memory (LSTM)</a></li>
<li><a href="/wiki/Gated_recurrent_unit" title="Gated recurrent unit">Gated recurrent unit (GRU)</a></li>
<li><a href="/wiki/Echo_state_network" title="Echo state network">Echo state network</a></li>
<li><a href="/wiki/Multilayer_perceptron" title="Multilayer perceptron">Multilayer perceptron (MLP)</a></li>
<li><a href="/wiki/Convolutional_neural_network" title="Convolutional neural network">Convolutional neural network</a></li>
<li><a href="/wiki/Residual_neural_network" title="Residual neural network">Residual neural network</a></li>
<li><a href="/wiki/Mamba_(deep_learning)" class="mw-redirect" title="Mamba (deep learning)">Mamba</a></li>
<li><a href="/wiki/Autoencoder" title="Autoencoder">Autoencoder</a></li>
<li><a href="/wiki/Variational_autoencoder" title="Variational autoencoder">Variational autoencoder (VAE)</a></li>
<li><a href="/wiki/Generative_adversarial_network" title="Generative adversarial network">Generative adversarial network (GAN)</a></li>
<li><a href="/wiki/Graph_neural_network" title="Graph neural network">Graph neural network</a></li></ul>
</div></td></tr><tr><td class="navbox-abovebelow" colspan="2"><div>
<ul><li><span class="noviewer" typeof="mw:File"><a href="/wiki/File:Symbol_portal_class.svg" class="mw-file-description" title="Portal"><img alt="" src="//upload.wikimedia.org/wikipedia/en/thumb/e/e2/Symbol_portal_class.svg/16px-Symbol_portal_class.svg.png" decoding="async" width="16" height="16" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/en/thumb/e/e2/Symbol_portal_class.svg/23px-Symbol_portal_class.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/e/e2/Symbol_portal_class.svg/31px-Symbol_portal_class.svg.png 2x" data-file-width="180" data-file-height="185" /></a></span> Portals
<ul><li><a href="/wiki/Portal:Computer_programming" title="Portal:Computer programming">Computer programming</a></li>
<li><a href="/wiki/Portal:Technology" title="Portal:Technology">Technology</a></li></ul></li>
<li><span class="noviewer" typeof="mw:File"><span title="Category"><img alt="" src="//upload.wikimedia.org/wikipedia/en/thumb/9/96/Symbol_category_class.svg/16px-Symbol_category_class.svg.png" decoding="async" width="16" height="16" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/en/thumb/9/96/Symbol_category_class.svg/23px-Symbol_category_class.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/9/96/Symbol_category_class.svg/31px-Symbol_category_class.svg.png 2x" data-file-width="180" data-file-height="185" /></span></span> Categories
<ul><li><a href="/wiki/Category:Artificial_neural_networks" title="Category:Artificial neural networks">Artificial neural networks</a></li>
<li><a href="/wiki/Category:Machine_learning" title="Category:Machine learning">Machine learning</a></li></ul></li></ul>
</div></td></tr></tbody></table></div>
<p class="mw-empty-elt">
</p>
<!-- 
NewPP limit report
Parsed by mw2329
Cached time: 20240413172337
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.494 seconds
Real time usage: 0.699 seconds
Preprocessor visited node count: 2945/1000000
Post‐expand include size: 143150/2097152 bytes
Template argument size: 4027/2097152 bytes
Highest expansion depth: 14/100
Expensive parser function count: 3/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 125604/5000000 bytes
Lua time usage: 0.275/10.000 seconds
Lua memory usage: 5528790/52428800 bytes
Number of Wikibase entities loaded: 0/400
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  511.709      1 -total
 44.84%  229.434      1 Template:Reflist
 18.17%   93.003      1 Template:Machine_learning_bar
 17.05%   87.267      4 Template:Cite_arXiv
 14.65%   74.963      1 Template:Sidebar_with_collapsible_lists
 13.95%   71.377      1 Template:Short_description
 11.03%   56.447     11 Template:Cite_journal
  8.98%   45.936      5 Template:Navbox
  7.15%   36.589      2 Template:Pagetype
  6.47%   33.128      1 Template:Explain
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:47527969-0!canonical and timestamp 20240413172337 and revision id 1215655449. Rendering was triggered because: page-view
 -->
</div><!--esi <esi:include src="/esitest-fa8a495983347898/content" /> --><noscript><img src="https://login.wikimedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" width="1" height="1" style="border: none; position: absolute;"></noscript>
<div class="printfooter" data-nosnippet="">Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=Word2vec&amp;oldid=1215655449">https://en.wikipedia.org/w/index.php?title=Word2vec&amp;oldid=1215655449</a>"</div></div>
					<div id="catlinks" class="catlinks" data-mw="interface"><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="/wiki/Category:Free_science_software" title="Category:Free science software">Free science software</a></li><li><a href="/wiki/Category:Natural_language_processing_toolkits" title="Category:Natural language processing toolkits">Natural language processing toolkits</a></li><li><a href="/wiki/Category:Artificial_neural_networks" title="Category:Artificial neural networks">Artificial neural networks</a></li><li><a href="/wiki/Category:Semantic_relations" title="Category:Semantic relations">Semantic relations</a></li></ul></div><div id="mw-hidden-catlinks" class="mw-hidden-catlinks mw-hidden-cats-hidden">Hidden categories: <ul><li><a href="/wiki/Category:CS1_errors:_missing_periodical" title="Category:CS1 errors: missing periodical">CS1 errors: missing periodical</a></li><li><a href="/wiki/Category:Articles_with_short_description" title="Category:Articles with short description">Articles with short description</a></li><li><a href="/wiki/Category:Short_description_is_different_from_Wikidata" title="Category:Short description is different from Wikidata">Short description is different from Wikidata</a></li><li><a href="/wiki/Category:Wikipedia_articles_needing_clarification_from_February_2022" title="Category:Wikipedia articles needing clarification from February 2022">Wikipedia articles needing clarification from February 2022</a></li><li><a href="/wiki/Category:Use_dmy_dates_from_April_2017" title="Category:Use dmy dates from April 2017">Use dmy dates from April 2017</a></li></ul></div></div>
				</div>
			</main>
			
		</div>
		<div class="mw-footer-container">
			
<footer id="footer" class="mw-footer" role="contentinfo" >
	<ul id="footer-info">
	<li id="footer-info-lastmod"> This page was last edited on 26 March 2024, at 12:07<span class="anonymous-show">&#160;(UTC)</span>.</li>
	<li id="footer-info-copyright">Text is available under the <a rel="license" href="//en.wikipedia.org/wiki/Wikipedia:Text_of_the_Creative_Commons_Attribution-ShareAlike_4.0_International_License">Creative Commons Attribution-ShareAlike License 4.0</a><a rel="license" href="//en.wikipedia.org/wiki/Wikipedia:Text_of_the_Creative_Commons_Attribution-ShareAlike_4.0_International_License" style="display:none;"></a>;
additional terms may apply. By using this site, you agree to the <a href="//foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Terms_of_Use">Terms of Use</a> and <a href="//foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Privacy_policy">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a href="//www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
</ul>

	<ul id="footer-places">
	<li id="footer-places-privacy"><a href="https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Privacy_policy">Privacy policy</a></li>
	<li id="footer-places-about"><a href="/wiki/Wikipedia:About">About Wikipedia</a></li>
	<li id="footer-places-disclaimers"><a href="/wiki/Wikipedia:General_disclaimer">Disclaimers</a></li>
	<li id="footer-places-contact"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
	<li id="footer-places-wm-codeofconduct"><a href="https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Universal_Code_of_Conduct">Code of Conduct</a></li>
	<li id="footer-places-developers"><a href="https://developer.wikimedia.org">Developers</a></li>
	<li id="footer-places-statslink"><a href="https://stats.wikimedia.org/#/en.wikipedia.org">Statistics</a></li>
	<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Cookie_statement">Cookie statement</a></li>
	<li id="footer-places-mobileview"><a href="//en.m.wikipedia.org/w/index.php?title=Word2vec&amp;mobileaction=toggle_view_mobile" class="noprint stopMobileRedirectToggle">Mobile view</a></li>
</ul>

	<ul id="footer-icons" class="noprint">
	<li id="footer-copyrightico"><a href="https://wikimediafoundation.org/"><img src="/static/images/footer/wikimedia-button.png" srcset="/static/images/footer/wikimedia-button-1.5x.png 1.5x, /static/images/footer/wikimedia-button-2x.png 2x" width="88" height="31" alt="Wikimedia Foundation" loading="lazy" /></a></li>
	<li id="footer-poweredbyico"><a href="https://www.mediawiki.org/"><img src="/static/images/footer/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" srcset="/static/images/footer/poweredby_mediawiki_132x47.png 1.5x, /static/images/footer/poweredby_mediawiki_176x62.png 2x" width="88" height="31" loading="lazy"></a></li>
</ul>

</footer>

		</div>
	</div> 
</div> 
<div class="vector-settings" id="p-dock-bottom">
	<ul>
		<li>
		<button class="cdx-button cdx-button--icon-only vector-limited-width-toggle" id=""><span class="vector-icon mw-ui-icon-fullScreen mw-ui-icon-wikimedia-fullScreen"></span>

<span>Toggle limited content width</span>
</button>
</li>
	</ul>
</div>
<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgHostname":"mw1418","wgBackendResponseTime":165,"wgPageParseReport":{"limitreport":{"cputime":"0.494","walltime":"0.699","ppvisitednodes":{"value":2945,"limit":1000000},"postexpandincludesize":{"value":143150,"limit":2097152},"templateargumentsize":{"value":4027,"limit":2097152},"expansiondepth":{"value":14,"limit":100},"expensivefunctioncount":{"value":3,"limit":500},"unstrip-depth":{"value":1,"limit":20},"unstrip-size":{"value":125604,"limit":5000000},"entityaccesscount":{"value":0,"limit":400},"timingprofile":["100.00%  511.709      1 -total"," 44.84%  229.434      1 Template:Reflist"," 18.17%   93.003      1 Template:Machine_learning_bar"," 17.05%   87.267      4 Template:Cite_arXiv"," 14.65%   74.963      1 Template:Sidebar_with_collapsible_lists"," 13.95%   71.377      1 Template:Short_description"," 11.03%   56.447     11 Template:Cite_journal","  8.98%   45.936      5 Template:Navbox","  7.15%   36.589      2 Template:Pagetype","  6.47%   33.128      1 Template:Explain"]},"scribunto":{"limitreport-timeusage":{"value":"0.275","limit":"10.000"},"limitreport-memusage":{"value":5528790,"limit":52428800}},"cachereport":{"origin":"mw2329","timestamp":"20240413172337","ttl":2592000,"transientcontent":false}}});});</script>
<script type="application/ld+json">{"@context":"https:\/\/schema.org","@type":"Article","name":"Word2vec","url":"https:\/\/en.wikipedia.org\/wiki\/Word2vec","sameAs":"http:\/\/www.wikidata.org\/entity\/Q22673982","mainEntity":"http:\/\/www.wikidata.org\/entity\/Q22673982","author":{"@type":"Organization","name":"Contributors to Wikimedia projects"},"publisher":{"@type":"Organization","name":"Wikimedia Foundation, Inc.","logo":{"@type":"ImageObject","url":"https:\/\/www.wikimedia.org\/static\/images\/wmf-hor-googpub.png"}},"datePublished":"2015-08-14T22:22:48Z","dateModified":"2024-03-26T12:07:49Z","headline":"group of related models that are used to produce word embeddings"}</script>
</body>
</html>